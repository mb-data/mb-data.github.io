{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Khipo Docs O objetivo deste site \u00e9 servir como um hub de informa\u00e7\u00f5es para os desenvolvedores da Khipo. Aqui voc\u00ea deve encontrar tutoriais, resumos e conte\u00fados diversos que auxiliem na sua forma\u00e7\u00e3o como desenvolvedor(a). Fique \u00e0 vontade para contribuir atrav\u00e9s do GitHub! Criado por Arthur Olga e Filipe Borba .","title":"Home"},{"location":"#khipo-docs","text":"O objetivo deste site \u00e9 servir como um hub de informa\u00e7\u00f5es para os desenvolvedores da Khipo. Aqui voc\u00ea deve encontrar tutoriais, resumos e conte\u00fados diversos que auxiliem na sua forma\u00e7\u00e3o como desenvolvedor(a). Fique \u00e0 vontade para contribuir atrav\u00e9s do GitHub! Criado por Arthur Olga e Filipe Borba .","title":"Khipo Docs"},{"location":"Testes/Vis%C3%A3o%20Geral/","text":"Teste de Software Teste de software \u00e9 um m\u00e9todo para checar se o seu software cumpre os requisitos e est\u00e1 livre de Defeitos. Envolve a execu\u00e7\u00e3o de componentes do sistema usando ferramentas manuais ou automatizadas para avaliar uma ou mais partes de interesse. O intuito do teste de software \u00e9 identificar erros, lacunas ou requisitos faltantes em contraste aos requisitos de fato. A \u00e1rea de teste de software \u00e9 bastante extensa, tendo profissionais dedicados \u00e0 essa parte do processo de desenvolvimento de software. Geralmente, esse tipo de processo \u00e9 realizado por um time de QA (Quality Assurance ou Controle de Qualidade). Dois fundamentos para esse segmento s\u00e3o: N\u00e3o \u00e9 poss\u00edvel atingir 100% de automa\u00e7\u00e3o. N\u00e3o \u00e9 poss\u00edvel testar 100% da aplica\u00e7\u00e3o. Mesmo nas aplica\u00e7\u00f5es mais triviais podem ocorrer entradas e erros inesperados. O Mito Mito: Testar requer tempo! Eu n\u00e3o tenho tempo, preciso entregar tudo pra ontem. Eu sou o mestre, meu c\u00f3digo \u00e9 perfeito, eu n\u00e3o preciso de testes. A verdade \u00e9 que criar testes automatizados aumentam a velocidade de desenvolvimento. Veja o seguinte ciclo: Programadores acreditam que os Testes Manuais v\u00e3o capturar todos os erros e n\u00e3o executam Testes Unit\u00e1rios. Assim que as unidades/componentes s\u00e3o integrados, erros simples que poderiam ter sido facilmente encontrados e consertados em Testes Unit\u00e1rios levam mais tempo para serem identificados e consertados. Benef\u00edcios e Import\u00e2ncia Testes s\u00e3o importantes porque bugs e erros podem ser identificados de forma precoce e solucionados antes da entrega/lan\u00e7amento do produto. Um software testado de forma apropriada garante alguns benef\u00edcios: Efici\u00eancia de Custo: uma das maiores vantagens. Testar qualquer projeto de TI ajuda o time a economizar dinheiro no longo prazo. No caso de bugs serem detectados em est\u00e1gios iniciais de desenvolvimento, eles custam menos para serem consertados. Seguran\u00e7a: pessoas buscam produtos confi\u00e1veis. Testes ajudam a remover riscos e problemas mais cedo, trazendo mais credibilidade. Qualidade de Produto: testes garantem a qualidade do produto que ser\u00e1 lan\u00e7ado aos clientes. Satisfa\u00e7\u00e3o do Cliente: o objetivo prim\u00e1rio de qualquer produto \u00e9 dar satisfa\u00e7\u00e3o aos clientes. Testes garantem a melhor experi\u00eancia do usu\u00e1rio (livre de bugs e crashes), ainda, podendo ser potencializado por testes de UI/UX. Tipos de Teste de Software Testes Funcionais Teste Unit\u00e1rio Teste de Integra\u00e7\u00e3o Teste de Fuma\u00e7a UAT (User Acceptance Testing) Teste de Globaliza\u00e7\u00e3o (tradu\u00e7\u00e3o) Teste de Interoperabilidade Entre outros Testes n\u00e3o-funcionais Performance Resili\u00eancia Carga Escalabilidade Usabilidade Entre outros Testes de Manuten\u00e7\u00e3o Manuten\u00e7\u00e3o Regress\u00e3o Principais testes Teste Unit\u00e1rio Testes unit\u00e1rios visam testar componentes individuais de uma aplica\u00e7\u00e3o. O intuito \u00e9 validar se cada unidade de um c\u00f3digo funciona como esperado, isolando uma parte do c\u00f3digo e verificando sua corretude. Esses testes s\u00e3o realizados na etapa de desenvolvimento da aplica\u00e7\u00e3o pelos desenvolvedores. Uma unidade ou componente de c\u00f3digo pode ser uma fun\u00e7\u00e3o, m\u00e9todo, procedimento, m\u00f3dulo ou objeto. Teste de Integra\u00e7\u00e3o Testes de integra\u00e7\u00e3o visam testar dois ou mais componentes como um grupo. Um projeto de software usual possui diversos m\u00f3dulos, codificados por diferentes programadores. O intuito desses testes s\u00e3o expor defeitos de intera\u00e7\u00e3o entre os m\u00f3dulos quando eles est\u00e3o integrados. Teste End-to-end Testes End-to-End (E2E ou ponta-a-ponta) buscam replicar o comportamento do usu\u00e1rio em um ambiente completo da aplica\u00e7\u00e3o. Ele verifica os v\u00e1rios fluxos que o usu\u00e1rio usa. Podem ser simples como carregar uma p\u00e1gina da web, fazer login ou ser t\u00e3o complexos quanto verificar notifica\u00e7\u00f5es de email, pagamentos online, etc... Esses testes s\u00e3o bastante \u00fateis, mas s\u00e3o custosos para executar e dif\u00edceis de manter quanto automatizados. \u00c9 recomendado ter alguns testes end-to-end chaves e recorrer mais aos testes de menor n\u00edvel para verificar mudan\u00e7as de quebra. Teste Manual Por fim, o bom e velho teste manual acaba sendo uma importante arma no arsenal. Como n\u00e3o \u00e9 poss\u00edvel chegar em 100% de automa\u00e7\u00e3o e nem inteligente, testes manuais visam reproduzir o que os testes end-to-end buscam. Verificar se os fluxos do usu\u00e1rio funcionam como esperado e n\u00e3o quebram quando houveram novas features ou refatora\u00e7\u00f5es. Esses testes acabam tomando tempo e podem ser cansativos de serem feitos, mas n\u00e3o podem ser negligenciados. Geralmente testes manuais s\u00e3o testes explorat\u00f3rios, ou seja, visam buscar erros n\u00e3o-\u00f3bvios na aplica\u00e7\u00e3o. Pir\u00e2mide de Teste A Pir\u00e2mide de Teste \u00e9 um framework que pode ajudar tanto desenvolvedores quanto QAs para criar softwares de alta qualidade. Isso reduz o tempo necess\u00e1rio para desenvolvedores identificarem se uma mudan\u00e7a introduzida quebrou o c\u00f3digo. Tamb\u00e9m \u00e9 \u00fatil para construir uma su\u00edte de teste mais consistente. A ideia da pir\u00e2mide \u00e9 que os testes unit\u00e1rios sejam a base da su\u00edte de testes, pois tomam menos tempo para serem escritos e rodam com mais frequ\u00eancia, al\u00e9m de proverem feedback ao desenvolvedor com mais rapidez. Os testes de integra\u00e7\u00e3o ficam num n\u00edvel superior, pois precisam testar duas ou mais unidades, tomando mais tempo para serem escritos, precisam de um ambiente para serem rodados, podem comunicar com aplica\u00e7\u00f5es externas e, por fim, tomam mais tempo do que testes unit\u00e1rios para rodar. Por \u00faltimo, no topo da pir\u00e2mide, os testes End-to-End tomam mais tempo para rodar, serem escritos e possuem depend\u00eancias, al\u00e9m de serem complexos de serem executados. Geralmente os testes End-to-End s\u00e3o substitu\u00eddos por testes manuais, portanto acabam sendo o \u00faltimo passo da su\u00edte de testes. Fontes https://www.guru99.com/software-testing-introduction-importance.html#5 https://www.guru99.com/application-testing.html https://www.guru99.com/frontend-testing-vs-backend-testing.html https://www.browserstack.com/guide/testing-pyramid-for-test-automation https://www.guru99.com/functional-testing.html https://www.guru99.com/manual-testing.html https://www.guru99.com/unit-testing-guide.html#9 https://medium.com/aperto-an-ibm-company/unit-testing-in-agile-web-projects-4db5547a733b https://www.digite.com/agile/unit-testing/ https://www.browserstack.com/guide/front-end-testing https://www.guru99.com/integration-testing.html https://www.atlassian.com/continuous-delivery/software-testing/types-of-software-testing","title":"Vis\u00e3o Geral"},{"location":"Testes/Vis%C3%A3o%20Geral/#teste-de-software","text":"Teste de software \u00e9 um m\u00e9todo para checar se o seu software cumpre os requisitos e est\u00e1 livre de Defeitos. Envolve a execu\u00e7\u00e3o de componentes do sistema usando ferramentas manuais ou automatizadas para avaliar uma ou mais partes de interesse. O intuito do teste de software \u00e9 identificar erros, lacunas ou requisitos faltantes em contraste aos requisitos de fato. A \u00e1rea de teste de software \u00e9 bastante extensa, tendo profissionais dedicados \u00e0 essa parte do processo de desenvolvimento de software. Geralmente, esse tipo de processo \u00e9 realizado por um time de QA (Quality Assurance ou Controle de Qualidade). Dois fundamentos para esse segmento s\u00e3o: N\u00e3o \u00e9 poss\u00edvel atingir 100% de automa\u00e7\u00e3o. N\u00e3o \u00e9 poss\u00edvel testar 100% da aplica\u00e7\u00e3o. Mesmo nas aplica\u00e7\u00f5es mais triviais podem ocorrer entradas e erros inesperados.","title":"Teste de Software"},{"location":"Testes/Vis%C3%A3o%20Geral/#o-mito","text":"Mito: Testar requer tempo! Eu n\u00e3o tenho tempo, preciso entregar tudo pra ontem. Eu sou o mestre, meu c\u00f3digo \u00e9 perfeito, eu n\u00e3o preciso de testes. A verdade \u00e9 que criar testes automatizados aumentam a velocidade de desenvolvimento. Veja o seguinte ciclo: Programadores acreditam que os Testes Manuais v\u00e3o capturar todos os erros e n\u00e3o executam Testes Unit\u00e1rios. Assim que as unidades/componentes s\u00e3o integrados, erros simples que poderiam ter sido facilmente encontrados e consertados em Testes Unit\u00e1rios levam mais tempo para serem identificados e consertados.","title":"O Mito"},{"location":"Testes/Vis%C3%A3o%20Geral/#beneficios-e-importancia","text":"Testes s\u00e3o importantes porque bugs e erros podem ser identificados de forma precoce e solucionados antes da entrega/lan\u00e7amento do produto. Um software testado de forma apropriada garante alguns benef\u00edcios: Efici\u00eancia de Custo: uma das maiores vantagens. Testar qualquer projeto de TI ajuda o time a economizar dinheiro no longo prazo. No caso de bugs serem detectados em est\u00e1gios iniciais de desenvolvimento, eles custam menos para serem consertados. Seguran\u00e7a: pessoas buscam produtos confi\u00e1veis. Testes ajudam a remover riscos e problemas mais cedo, trazendo mais credibilidade. Qualidade de Produto: testes garantem a qualidade do produto que ser\u00e1 lan\u00e7ado aos clientes. Satisfa\u00e7\u00e3o do Cliente: o objetivo prim\u00e1rio de qualquer produto \u00e9 dar satisfa\u00e7\u00e3o aos clientes. Testes garantem a melhor experi\u00eancia do usu\u00e1rio (livre de bugs e crashes), ainda, podendo ser potencializado por testes de UI/UX.","title":"Benef\u00edcios e Import\u00e2ncia"},{"location":"Testes/Vis%C3%A3o%20Geral/#tipos-de-teste-de-software","text":"","title":"Tipos de Teste de Software"},{"location":"Testes/Vis%C3%A3o%20Geral/#testes-funcionais","text":"Teste Unit\u00e1rio Teste de Integra\u00e7\u00e3o Teste de Fuma\u00e7a UAT (User Acceptance Testing) Teste de Globaliza\u00e7\u00e3o (tradu\u00e7\u00e3o) Teste de Interoperabilidade Entre outros","title":"Testes Funcionais"},{"location":"Testes/Vis%C3%A3o%20Geral/#testes-nao-funcionais","text":"Performance Resili\u00eancia Carga Escalabilidade Usabilidade Entre outros","title":"Testes n\u00e3o-funcionais"},{"location":"Testes/Vis%C3%A3o%20Geral/#testes-de-manutencao","text":"Manuten\u00e7\u00e3o Regress\u00e3o","title":"Testes de Manuten\u00e7\u00e3o"},{"location":"Testes/Vis%C3%A3o%20Geral/#principais-testes","text":"","title":"Principais testes"},{"location":"Testes/Vis%C3%A3o%20Geral/#teste-unitario","text":"Testes unit\u00e1rios visam testar componentes individuais de uma aplica\u00e7\u00e3o. O intuito \u00e9 validar se cada unidade de um c\u00f3digo funciona como esperado, isolando uma parte do c\u00f3digo e verificando sua corretude. Esses testes s\u00e3o realizados na etapa de desenvolvimento da aplica\u00e7\u00e3o pelos desenvolvedores. Uma unidade ou componente de c\u00f3digo pode ser uma fun\u00e7\u00e3o, m\u00e9todo, procedimento, m\u00f3dulo ou objeto.","title":"Teste Unit\u00e1rio"},{"location":"Testes/Vis%C3%A3o%20Geral/#teste-de-integracao","text":"Testes de integra\u00e7\u00e3o visam testar dois ou mais componentes como um grupo. Um projeto de software usual possui diversos m\u00f3dulos, codificados por diferentes programadores. O intuito desses testes s\u00e3o expor defeitos de intera\u00e7\u00e3o entre os m\u00f3dulos quando eles est\u00e3o integrados.","title":"Teste de Integra\u00e7\u00e3o"},{"location":"Testes/Vis%C3%A3o%20Geral/#teste-end-to-end","text":"Testes End-to-End (E2E ou ponta-a-ponta) buscam replicar o comportamento do usu\u00e1rio em um ambiente completo da aplica\u00e7\u00e3o. Ele verifica os v\u00e1rios fluxos que o usu\u00e1rio usa. Podem ser simples como carregar uma p\u00e1gina da web, fazer login ou ser t\u00e3o complexos quanto verificar notifica\u00e7\u00f5es de email, pagamentos online, etc... Esses testes s\u00e3o bastante \u00fateis, mas s\u00e3o custosos para executar e dif\u00edceis de manter quanto automatizados. \u00c9 recomendado ter alguns testes end-to-end chaves e recorrer mais aos testes de menor n\u00edvel para verificar mudan\u00e7as de quebra.","title":"Teste End-to-end"},{"location":"Testes/Vis%C3%A3o%20Geral/#teste-manual","text":"Por fim, o bom e velho teste manual acaba sendo uma importante arma no arsenal. Como n\u00e3o \u00e9 poss\u00edvel chegar em 100% de automa\u00e7\u00e3o e nem inteligente, testes manuais visam reproduzir o que os testes end-to-end buscam. Verificar se os fluxos do usu\u00e1rio funcionam como esperado e n\u00e3o quebram quando houveram novas features ou refatora\u00e7\u00f5es. Esses testes acabam tomando tempo e podem ser cansativos de serem feitos, mas n\u00e3o podem ser negligenciados. Geralmente testes manuais s\u00e3o testes explorat\u00f3rios, ou seja, visam buscar erros n\u00e3o-\u00f3bvios na aplica\u00e7\u00e3o.","title":"Teste Manual"},{"location":"Testes/Vis%C3%A3o%20Geral/#piramide-de-teste","text":"A Pir\u00e2mide de Teste \u00e9 um framework que pode ajudar tanto desenvolvedores quanto QAs para criar softwares de alta qualidade. Isso reduz o tempo necess\u00e1rio para desenvolvedores identificarem se uma mudan\u00e7a introduzida quebrou o c\u00f3digo. Tamb\u00e9m \u00e9 \u00fatil para construir uma su\u00edte de teste mais consistente. A ideia da pir\u00e2mide \u00e9 que os testes unit\u00e1rios sejam a base da su\u00edte de testes, pois tomam menos tempo para serem escritos e rodam com mais frequ\u00eancia, al\u00e9m de proverem feedback ao desenvolvedor com mais rapidez. Os testes de integra\u00e7\u00e3o ficam num n\u00edvel superior, pois precisam testar duas ou mais unidades, tomando mais tempo para serem escritos, precisam de um ambiente para serem rodados, podem comunicar com aplica\u00e7\u00f5es externas e, por fim, tomam mais tempo do que testes unit\u00e1rios para rodar. Por \u00faltimo, no topo da pir\u00e2mide, os testes End-to-End tomam mais tempo para rodar, serem escritos e possuem depend\u00eancias, al\u00e9m de serem complexos de serem executados. Geralmente os testes End-to-End s\u00e3o substitu\u00eddos por testes manuais, portanto acabam sendo o \u00faltimo passo da su\u00edte de testes.","title":"Pir\u00e2mide de Teste"},{"location":"Testes/Vis%C3%A3o%20Geral/#fontes","text":"https://www.guru99.com/software-testing-introduction-importance.html#5 https://www.guru99.com/application-testing.html https://www.guru99.com/frontend-testing-vs-backend-testing.html https://www.browserstack.com/guide/testing-pyramid-for-test-automation https://www.guru99.com/functional-testing.html https://www.guru99.com/manual-testing.html https://www.guru99.com/unit-testing-guide.html#9 https://medium.com/aperto-an-ibm-company/unit-testing-in-agile-web-projects-4db5547a733b https://www.digite.com/agile/unit-testing/ https://www.browserstack.com/guide/front-end-testing https://www.guru99.com/integration-testing.html https://www.atlassian.com/continuous-delivery/software-testing/types-of-software-testing","title":"Fontes"},{"location":"old/glossary/","text":"Contents A short description of each topic or subtopic. MLOps Theory: A brief introduction to the main Principles of MLOps: Data and Model Versioning , Feature Management and Storing , Automation of Pipelines and Processes , CI/CD for Machine Learning and Continuous Monitoring of Models . As well as Common Tools used to address each of those points. Implementations Guide Introduction Tools and Project Structure: Introduction to the project structure and tools that will be used. Starting a New Project with Cookiecutter: Introduction to Cookiecutter and how to create a new project based on our template. Environment Setting Up the IBM Environment with Terraform: Using Terraform to set up the IBM Environment via code. Managing the deployment space: How to manage IBM's tools with Terraform. Versioning What is DVC?: Introduction to DVC, installation and how to setup remote storage. Data Versioning: Working with DVC to version data and models. Working with Pipelines: Creating and reproducing pipelines for training and evaluating models. Deployment Deployment with Watson Machine Learning: Using Watson ML API to deploy models as an online web service. CI/CD for Machine Learning Continuous Integration with CML and Github Actions: Introduction to CML and GitHub Actions and how to create automatic testing and reportng workflows. Continous Delivery with CML, Github Actions and Watson ML: Using CML and GitHub Actions create automatic deployments for every new release. Monitoring Monitoring with IBM OpenScale: Setting up OpenScale environment, creating monitors, evaluating the model and explaining predictions. Project Workflow Project Workflow: Demonstrating how to use the complete pipeline in a development cycle we created with commands and videos.","title":"Contents"},{"location":"old/glossary/#contents","text":"A short description of each topic or subtopic. MLOps Theory: A brief introduction to the main Principles of MLOps: Data and Model Versioning , Feature Management and Storing , Automation of Pipelines and Processes , CI/CD for Machine Learning and Continuous Monitoring of Models . As well as Common Tools used to address each of those points. Implementations Guide Introduction Tools and Project Structure: Introduction to the project structure and tools that will be used. Starting a New Project with Cookiecutter: Introduction to Cookiecutter and how to create a new project based on our template. Environment Setting Up the IBM Environment with Terraform: Using Terraform to set up the IBM Environment via code. Managing the deployment space: How to manage IBM's tools with Terraform. Versioning What is DVC?: Introduction to DVC, installation and how to setup remote storage. Data Versioning: Working with DVC to version data and models. Working with Pipelines: Creating and reproducing pipelines for training and evaluating models. Deployment Deployment with Watson Machine Learning: Using Watson ML API to deploy models as an online web service. CI/CD for Machine Learning Continuous Integration with CML and Github Actions: Introduction to CML and GitHub Actions and how to create automatic testing and reportng workflows. Continous Delivery with CML, Github Actions and Watson ML: Using CML and GitHub Actions create automatic deployments for every new release. Monitoring Monitoring with IBM OpenScale: Setting up OpenScale environment, creating monitors, evaluating the model and explaining predictions. Project Workflow Project Workflow: Demonstrating how to use the complete pipeline in a development cycle we created with commands and videos.","title":"Contents"},{"location":"old/CICD/cml_deploy/","text":"Continous Delivery with CML, Github Actions and Watson ML To deploy or update the deployment without the need to manually do so by the Watson ML UI or by running a script, we use GItHub Actions to run a workflow everytime we make a new release and then deploy the model that way. Creating a Release On GitHub, go to the main page of the repository. To the right of the page, click on Releases or Latest release . Click Draft a new release on the top right. Type the version of the new release. Click on Publish release . After that, Actions will trigger and the model on Watson ML, next we will explain how to implement this workflow. For more information regarding releases with GitHub, refer to this article . Git Actions The workflow downloads the data from dvc the uses credentials, to understand all the previous steps, refer to the last section where we explained step by step what is CML and how to implement workflows . The complete workflow can be found here . First we set the workflow to trigger every time a new release is created: name : model-deploy-on-release on : release : types : - 'created' Then we execute the following steps: run : | # Install requirements pip install -r requirements.txt # Pull data & run-cache from S3 and reproduce pipeline dvc pull --run-cache dvc repro # Decrypt credentials file gpg --quiet --batch --yes --decrypt --passphrase=\"$CRED_SECRET\" --output credentials.yaml credentials.yaml.gpg # Check if there is a deployment already, if positive update it, otherwise deploys it for the first time ./src/scripts/Scripts/git_release_pipeline.sh Installing requirements pip install -r requirements.txt Pull the versioned data and reproduce the full pipeline of training and evaluation dvc pull --run-cache dvc repro Decrypting Credentials File gpg --quiet --batch --yes --decrypt --passphrase=\"$CRED_SECRET\" --output credentials.yaml credentials.yaml.gpg Run a script to Deploy or Update the Deployment ./src/scripts/Scripts/git_release_pipeline.sh This is the following bash script: #!/bin/sh if ! python3 ./src/scripts/Pipelines/git_release_pipeline.py ./ then echo \" Model already has been deployed, updating it\" python3 ./src/scripts/Pipelines/model_update_pipeline.py ./models/model.joblib ./ ./credentials.yaml python3 ./src/scripts/Pipelines/model_update_deployment_pipeline.py ./ ./credentials.yaml else echo \" Deploying model for the first time\" python3 ./src/scripts/Pipelines/model_deploy_pipeline.py ./models/model.joblib ./ ./credentials.yaml fi First we check if there is a model UID in the metadata.yaml . If positive, we consider the model has already been deployed, and the we run scripts to update the model and the deployment . To find more details on how we use scripts to deploy the model on Watson Machine Learning you can go to this page from our guide.","title":"Continous Delivery with CML, Github Actions and Watson ML"},{"location":"old/CICD/cml_deploy/#continous-delivery-with-cml-github-actions-and-watson-ml","text":"To deploy or update the deployment without the need to manually do so by the Watson ML UI or by running a script, we use GItHub Actions to run a workflow everytime we make a new release and then deploy the model that way.","title":"Continous Delivery with CML, Github Actions and Watson ML"},{"location":"old/CICD/cml_deploy/#creating-a-release","text":"On GitHub, go to the main page of the repository. To the right of the page, click on Releases or Latest release . Click Draft a new release on the top right. Type the version of the new release. Click on Publish release . After that, Actions will trigger and the model on Watson ML, next we will explain how to implement this workflow. For more information regarding releases with GitHub, refer to this article .","title":"Creating a Release"},{"location":"old/CICD/cml_deploy/#git-actions","text":"The workflow downloads the data from dvc the uses credentials, to understand all the previous steps, refer to the last section where we explained step by step what is CML and how to implement workflows . The complete workflow can be found here . First we set the workflow to trigger every time a new release is created: name : model-deploy-on-release on : release : types : - 'created' Then we execute the following steps: run : | # Install requirements pip install -r requirements.txt # Pull data & run-cache from S3 and reproduce pipeline dvc pull --run-cache dvc repro # Decrypt credentials file gpg --quiet --batch --yes --decrypt --passphrase=\"$CRED_SECRET\" --output credentials.yaml credentials.yaml.gpg # Check if there is a deployment already, if positive update it, otherwise deploys it for the first time ./src/scripts/Scripts/git_release_pipeline.sh Installing requirements pip install -r requirements.txt Pull the versioned data and reproduce the full pipeline of training and evaluation dvc pull --run-cache dvc repro Decrypting Credentials File gpg --quiet --batch --yes --decrypt --passphrase=\"$CRED_SECRET\" --output credentials.yaml credentials.yaml.gpg Run a script to Deploy or Update the Deployment ./src/scripts/Scripts/git_release_pipeline.sh This is the following bash script: #!/bin/sh if ! python3 ./src/scripts/Pipelines/git_release_pipeline.py ./ then echo \" Model already has been deployed, updating it\" python3 ./src/scripts/Pipelines/model_update_pipeline.py ./models/model.joblib ./ ./credentials.yaml python3 ./src/scripts/Pipelines/model_update_deployment_pipeline.py ./ ./credentials.yaml else echo \" Deploying model for the first time\" python3 ./src/scripts/Pipelines/model_deploy_pipeline.py ./models/model.joblib ./ ./credentials.yaml fi First we check if there is a model UID in the metadata.yaml . If positive, we consider the model has already been deployed, and the we run scripts to update the model and the deployment . To find more details on how we use scripts to deploy the model on Watson Machine Learning you can go to this page from our guide.","title":"Git Actions"},{"location":"old/CICD/cml_testing/","text":"Continuous Integration with CML and Github Actions What is CML? CML, which stands by Continuous Machine Learning, it's an open-source library focused on delivering CI/CD for machine learning projects. Its principles includes: GitFlow: Using Git workflow for managing experiments alongside DVC versioning data and models. Auto reports for experiments: CML can generate reports in pull requests with metrics and plots helping the team to make informed and data-driven decisions Technology Agnostics: Build pipelines with Github or Gitlab and run experiments with any cloud service. Since we are using Github as our Git repository, Github Actions will be used to set up CML. Github Actions is managed by Github, so there is no need to worry about scale and operate the infrastructure just as other tools like Jenkins. Testing With Github Actions First, we will make sure our tests created at the Testing with Pytest and Black section are being executed every time that there is a new push to the repository on Github. This important in order to achieve redundancy in testing the project, and avoid making sure the code runs without errors on any environment and not just the developer's computer. To do this is very simple. If you used cookiecutter you should already have a file named test_on_push.yaml at .github/workflows/ folder. The content of the file should be: name : Python Package and Test on : [ push ] jobs : build : runs-on : ubuntu-latest strategy : matrix : python-version : [ 3.6 ] steps : - uses : actions/checkout@v2 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install pytest black if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - name : Test with pytest run : | pytest - name : Python Black run : | black . --check Now, we should take a look into those commands. Choosing when this action will run on : [ push ] Makes it run every time there is a push on the repository Setting up a Github Instance to run it runs-on : ubuntu-latest Github will setup a free Ubuntu instance for us, using the latest official release. Chosing the right version of Python matrix : python-version : [ 3.6 ] Installing test requirements python -m pip install --upgrade pip pip install pytest black Installing project requirements if [ -f requirements.txt ]; then pip install -r requirements.txt; fi Running Pytest - name : Test with pytest run : | pytest Running Black check - name : Python Black run : | black . --check This checks that every Python file is formatted according to Black. If all tests pass the commit pushed to the repository will receive a green check show it has no errors. Pipeline Test After setting up our test on push, let's focus on reproducing our pipeline on push and generate an automated report in pull requests to compare the experiment with the model at the main branch. Configuring Credentials To reproduce our experiment pipeline, we need to start pulling our data versioned by DVC. On the other hand, Github Actions will execute our pipeline inside a container pre-configured by CML that doesn't have our IBM credentials to pull the data from IBM COS. So, to solve this, let's configure our IBM credentials in Github Secrets. Github Secrets is a repository tool that encrypts credentials to be used as environment variables in the project. Go to the repository settings On the left menu, click on 'Secrets' Click on 'New repository secret' Add both AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY Adding Train and Evaluate Workflow Just like we did at the Testing Setup , let's create a file named train_evaluate.yaml at the .github/workflows/ folder, which content should be: name : model-training-evaluate on : [ push ] jobs : run : runs-on : [ ubuntu-latest ] container : docker://dvcorg/cml-py3:latest steps : - uses : actions/checkout@v2 - name : 'Train and Evaluate model' shell : bash env : repo_token : ${{ secrets.GITHUB_TOKEN }} AWS_ACCESS_KEY_ID : ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY : ${{ secrets.AWS_SECRET_ACCESS_KEY }} run : | # Install requirements pip install -r requirements.txt # Pull data & run-cache from IBM COS and reproduce pipeline dvc pull --run-cache dvc repro # Report metrics echo \"## Metrics\" >> report.md git fetch --prune dvc metrics diff master --show-md >> report.md # Publish ROC Curve and echo -e \"## Plots\\n### ROC Curve\" >> report.md cml-publish ./results/roc_curve.png --md >> report.md echo -e \"\\n### Precision and Recall Curve\" >> report.md cml-publish ./results/precision_recall_curve.png --md >> report.md cml-send-comment report.md Let's dig into each command: Setting up a CML pre-configured container container : docker://dvcorg/cml-py3:latest Setting up environment credentials for IBM COS env : repo_token : ${{ secrets.GITHUB_TOKEN }} AWS_ACCESS_KEY_ID : ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY : ${{ secrets.AWS_SECRET_ACCESS_KEY}} Installing requirements pip install -r requirements.txt Pull the versioned data and reproduce the full pipeline of training and evaluation dvc pull --run-cache dvc repro Formatting report section tittles echo \"## Metrics\" >> report.md echo -e \"## Plots\\n### ROC Curve\" >> report.md echo -e \"\\n### Precision and Recall Curve\" >> report.md Comparing metrics and publishing it to the report dvc metrics diff master --show-md >> report.md Publishing figures from the experiment to the report cml-publish ./results/roc_curve.png --md >> report.md cml-publish ./results/precision_recall_curve.png --md >> report.md Return the final report formatted as a comment on the Commit or Pull Request cml-send-comment report.md The Report should look like the following: (.zoom)","title":"Continuous Integration with CML and Github Actions"},{"location":"old/CICD/cml_testing/#continuous-integration-with-cml-and-github-actions","text":"","title":"Continuous Integration with CML and Github Actions"},{"location":"old/CICD/cml_testing/#what-is-cml","text":"CML, which stands by Continuous Machine Learning, it's an open-source library focused on delivering CI/CD for machine learning projects. Its principles includes: GitFlow: Using Git workflow for managing experiments alongside DVC versioning data and models. Auto reports for experiments: CML can generate reports in pull requests with metrics and plots helping the team to make informed and data-driven decisions Technology Agnostics: Build pipelines with Github or Gitlab and run experiments with any cloud service. Since we are using Github as our Git repository, Github Actions will be used to set up CML. Github Actions is managed by Github, so there is no need to worry about scale and operate the infrastructure just as other tools like Jenkins.","title":"What is CML?"},{"location":"old/CICD/cml_testing/#testing-with-github-actions","text":"First, we will make sure our tests created at the Testing with Pytest and Black section are being executed every time that there is a new push to the repository on Github. This important in order to achieve redundancy in testing the project, and avoid making sure the code runs without errors on any environment and not just the developer's computer. To do this is very simple. If you used cookiecutter you should already have a file named test_on_push.yaml at .github/workflows/ folder. The content of the file should be: name : Python Package and Test on : [ push ] jobs : build : runs-on : ubuntu-latest strategy : matrix : python-version : [ 3.6 ] steps : - uses : actions/checkout@v2 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install pytest black if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - name : Test with pytest run : | pytest - name : Python Black run : | black . --check Now, we should take a look into those commands. Choosing when this action will run on : [ push ] Makes it run every time there is a push on the repository Setting up a Github Instance to run it runs-on : ubuntu-latest Github will setup a free Ubuntu instance for us, using the latest official release. Chosing the right version of Python matrix : python-version : [ 3.6 ] Installing test requirements python -m pip install --upgrade pip pip install pytest black Installing project requirements if [ -f requirements.txt ]; then pip install -r requirements.txt; fi Running Pytest - name : Test with pytest run : | pytest Running Black check - name : Python Black run : | black . --check This checks that every Python file is formatted according to Black. If all tests pass the commit pushed to the repository will receive a green check show it has no errors.","title":"Testing With Github Actions"},{"location":"old/CICD/cml_testing/#pipeline-test","text":"After setting up our test on push, let's focus on reproducing our pipeline on push and generate an automated report in pull requests to compare the experiment with the model at the main branch.","title":"Pipeline Test"},{"location":"old/CICD/cml_testing/#configuring-credentials","text":"To reproduce our experiment pipeline, we need to start pulling our data versioned by DVC. On the other hand, Github Actions will execute our pipeline inside a container pre-configured by CML that doesn't have our IBM credentials to pull the data from IBM COS. So, to solve this, let's configure our IBM credentials in Github Secrets. Github Secrets is a repository tool that encrypts credentials to be used as environment variables in the project. Go to the repository settings On the left menu, click on 'Secrets' Click on 'New repository secret' Add both AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY","title":"Configuring Credentials"},{"location":"old/CICD/cml_testing/#adding-train-and-evaluate-workflow","text":"Just like we did at the Testing Setup , let's create a file named train_evaluate.yaml at the .github/workflows/ folder, which content should be: name : model-training-evaluate on : [ push ] jobs : run : runs-on : [ ubuntu-latest ] container : docker://dvcorg/cml-py3:latest steps : - uses : actions/checkout@v2 - name : 'Train and Evaluate model' shell : bash env : repo_token : ${{ secrets.GITHUB_TOKEN }} AWS_ACCESS_KEY_ID : ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY : ${{ secrets.AWS_SECRET_ACCESS_KEY }} run : | # Install requirements pip install -r requirements.txt # Pull data & run-cache from IBM COS and reproduce pipeline dvc pull --run-cache dvc repro # Report metrics echo \"## Metrics\" >> report.md git fetch --prune dvc metrics diff master --show-md >> report.md # Publish ROC Curve and echo -e \"## Plots\\n### ROC Curve\" >> report.md cml-publish ./results/roc_curve.png --md >> report.md echo -e \"\\n### Precision and Recall Curve\" >> report.md cml-publish ./results/precision_recall_curve.png --md >> report.md cml-send-comment report.md Let's dig into each command: Setting up a CML pre-configured container container : docker://dvcorg/cml-py3:latest Setting up environment credentials for IBM COS env : repo_token : ${{ secrets.GITHUB_TOKEN }} AWS_ACCESS_KEY_ID : ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY : ${{ secrets.AWS_SECRET_ACCESS_KEY}} Installing requirements pip install -r requirements.txt Pull the versioned data and reproduce the full pipeline of training and evaluation dvc pull --run-cache dvc repro Formatting report section tittles echo \"## Metrics\" >> report.md echo -e \"## Plots\\n### ROC Curve\" >> report.md echo -e \"\\n### Precision and Recall Curve\" >> report.md Comparing metrics and publishing it to the report dvc metrics diff master --show-md >> report.md Publishing figures from the experiment to the report cml-publish ./results/roc_curve.png --md >> report.md cml-publish ./results/precision_recall_curve.png --md >> report.md Return the final report formatted as a comment on the Commit or Pull Request cml-send-comment report.md The Report should look like the following: (.zoom)","title":"Adding Train and Evaluate Workflow"},{"location":"old/CICD/tests/","text":"Implementing Tests Why Testing Your Project? In the software development industry it has become a good practice to test your code and try achieving the most possible coverage. This practice helps with making a project that is maintainable, reliable and avoids problems like shipping new versions that break old features. It is important to follow the same practice on the MLOps spectrum by testing the code, the model and even the data. Making tests is somewhat very particular to the project, so this section will show a simple implementation of a testing pipeline using Pytest and Pre-commit, which should later be filled with tests that comprehend the specific project needs. Types of testing It is important to notice that many projects will have different needs in terms of testing, so this division may not be precise for all projects, but in broad terms the main types of testing in MLOps are: Software Testing: Comprehends tests that make sure the code follows the project requirements. This is the type of tests normally implemented in DevOps, such as Unit Testing, Integration Testing, System Testing and Acceptance Testing. Model Testing: Comprehends tests that define that the model is working fine, such as testing that it can be trained, or that it can obtain a minimum score at some evaluation. Data Testing: Comprehends tests that check for data correctness. It is heavily dependant on the project requirements and can be focused on securing that the analyzed dataset follows a schema, contains enough data, and others. Many thing can be tested here depending on the necessities of the team. Using Pytest First, we will be using a Python package called Pytest, which is a very popular choice for regular Software testing and can be used to implement all sorts of tests. We are going to build a simple a Pytest file that shows an example of testing the whole project and the team can later add more tests specific for their use cases. In the src/tests/ folder you should see a Python file with a custom name with this content: import pytest def capital_case ( x ): return x . capitalize () def test_capital_case (): assert capital_case ( \"semaphore\" ) == \"Semaphore\" This is a simple file that imports the Pytest package and shows a simple test. capital_case() is a function that could be defined in any place of the entire project, and test_capital_case() is a test function, which means it should be used to test the capital case function. To run all tests: pytest By running pytest on the command line Python finds all the files that have a function starting with the test_ prefix and run them. This means that capital_case() is not run directly and is only running inside test_capital_case() . Configuring Pre-commit Pre-commit is a tool that enables us to use Git Hooks, a particular feature of Git itself, not necessarily Github or GitLab, that enables us to run scripts and commands when performing certain actions such as commit, push, pull, etc. This CLI tool enables us to create a Git Hook easily to test our code before each commit, making it harder for a developer to add code to the Github repository with errors. You should have a .pre-commit-config.yaml file at the root directory with: --- repos : - repo : https://github.com/ambv/black rev : 20.8b1 hooks : - id : black language_version : python3 - repo : local hooks : - id : python-tests name : pytests entry : pytest src/tests language : python additional_dependencies : [ pre-commit , pytest , pandas , sklearn , matplotlib ] always_run : true pass_filenames : false This file can be used to configure a Git Hook that runs Pytest and Black Python formatter before each commit. So, to install this Git Hook use: pre-commit install Now, after you commit some changes in the repository you should see a message stating that all tests passed. If some of the tests didn't pass, the Git Hook won't let you commit until you fix the errors. $ git commit -m \"Example commit\" black....................................................................Passed pytest-check.............................................................Passed Adding Software Testing Now we are going to write some more tests for the project. One of the most necessary parts of testing in this project is on the Preprocess Pipeline, which comprehends the highest number of functions. So here we will be implementing a test the covers those functions, their ability to read and write files, and that a reduced dataset can be preprocessed with no problems. Create a folder called preprocess/ inside tests/ , and inside this folder a file called test_preprocess.py and another folder called test_data/ , in which we are going to add a simple dataset file testWeatherAUS.csv with the first 2 lines of the real dataset. Now with this created, edit tests/preprocess/test_preprocess.py with some imports and an addition to path to be able to access the test dataset. import io import builtins import pytest import pandas as pd import sys import os # Parent Folder which contains test_data/ sys . path . append ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ )))) ) # Preprocess Python file which contains our functions to be tested import preprocess_data FILE_NAME = \"testWeatherAUS\" DATA_PATH = ( os . path . dirname ( os . path . realpath ( __file__ )) + \"/test_data/\" + FILE_NAME + \".csv\" ) PROCESSED_DATA_PATH = ( os . path . dirname ( os . path . realpath ( __file__ )) + \"/test_data/\" + FILE_NAME + \"_processed.csv\" ) Now, we are going to add unit testing for some of the functions inside preprocess_data.py . def test_count_nulls_by_line (): # Tests function that counts number of nulls by line on a dataframe data = pd . DataFrame ([[ 0 , 2 ], [ 0 , 1 ], [ 6 , None ]]) assert preprocess_data . count_nulls_by_line ( data ) . to_list () == [ 1 , 0 ] def test_null_percent (): # Tests function that gets the percentage of nulls by line on a dataframe data = pd . DataFrame ([[ 0 , 2 ], [ 1 , None ]]) assert preprocess_data . null_percent_by_line ( data ) . to_list () == [ 0.5 , 0 ] Now that we made some examples of unit testing, we should add tests that comprehend the full pipeline, in this case checking if it runs smoothly or returns an error. Pretty general test that can be used to avoid some basic mistakes. This test creates a testWeatherAUS_processed.csv file. This test is going to be important for the next tests, so we will add a mark of dependency, which is a Pytest feature that can be used to tell Python that a test function should be run before another. In this case, any function that marks this function as a dependency will run after this one finishes. @pytest . mark . dependency () def test_preprocess (): # Checks if running the preprocess function returns an error preprocess_data . preprocess_data ( DATA_PATH ) Adding tests that make sure the file is created and is possible to read, which comprehends some system errors. It is possible to see that these functions have marked dependencies, which will make them run after the functions marked. @pytest . mark . dependency ( depends = [ \"test_preprocess\" ]) def test_processed_file_created (): # Checks if the processed file was created during test_preprocess() and is accessible f = open ( PROCESSED_DATA_PATH ) @pytest . mark . dependency ( depends = [ \"test_processed_file_created\" ]) def test_processed_file_format (): # Checks if the processed file is in the correct format (.csv) and can be transformed in dataframe try : pd . read_csv ( PROCESSED_DATA_PATH ) except : raise RuntimeError ( \"Unable to open \" + PROCESSED_DATA_PATH + \" as dataframe\" ) Now that we have implemented our example of tests, we need to delete the file created, which is test_data/testWeatherAUS_processed.csv . A ligature is a feature of Pytest that makes it possible to run code before or after the tests, which are run in the yield call on the code. @pytest . fixture ( scope = \"session\" , autouse = True ) def cleanup ( request ): # Runs tests then cleans up the processed file yield try : os . remove ( PROCESSED_DATA_PATH ) except : pass Now, when you run a pytest , or `git commit which triggers pytest, all these tests should run. Adding More Test Files It is important to note that it is very important to tests the functions that are being used in the scripts of model handling and data handling, such as the function get_variables() in the model.py . Se we are going to follow the same structure and create a file tests/model/test_model.py . In this case we would like to test a large variety of different inputs, so we will be using pytest.mark.parametrize() which enables us to choose a large amount of inputs and expected results. import sys import os import pytest import pandas as pd # Parent Folder sys . path . append ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ )))) ) # Model Python file from model import get_variables FILE_NAME = \"testWeatherAUS\" PROCESSED_DATA_PATH = ( os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ ))) + \"/test_data/\" + FILE_NAME + \"_processed.csv\" ) @pytest . mark . parametrize ( \"expected_X,expected_y\" , [ ( { \"MinTemp\" : { 0 : 13.4 , 1 : 7.4 }, \"MaxTemp\" : { 0 : 22.9 , 1 : 25.1 }, \"Rainfall\" : { 0 : 0.6 , 1 : 0.0 }, \"WindGustSpeed\" : { 0 : 44 , 1 : 44 }, \"WindSpeed9am\" : { 0 : 20 , 1 : 4 }, \"WindSpeed3pm\" : { 0 : 24 , 1 : 22 }, \"Humidity9am\" : { 0 : 71 , 1 : 44 }, \"Humidity3pm\" : { 0 : 22 , 1 : 25 }, \"Pressure9am\" : { 0 : 1007.7 , 1 : 1010.6 }, \"Pressure3pm\" : { 0 : 1007.1 , 1 : 1007.8 }, \"Temp9am\" : { 0 : 16.9 , 1 : 17.2 }, \"Temp3pm\" : { 0 : 21.8 , 1 : 24.3 }, \"RainToday\" : { 0 : 0 , 1 : 0 }, \"WindGustDir_W\" : { 0 : 1 , 1 : 0 }, \"WindGustDir_WNW\" : { 0 : 0 , 1 : 1 }, \"WindDir9am_NNW\" : { 0 : 0 , 1 : 1 }, \"WindDir9am_W\" : { 0 : 1 , 1 : 0 }, \"WindDir3pm_WNW\" : { 0 : 1 , 1 : 0 }, \"WindDir3pm_WSW\" : { 0 : 0 , 1 : 1 }, }, [ 0 , 0 ], ) ], ) def test_get_variables ( expected_X , expected_y ): # Open CSV as DF data = pd . read_csv ( PROCESSED_DATA_PATH ) # Run Function X , y = get_variables ( data , \"RainTomorrow\" ) assert ( X . to_dict (), y . to_list ()) == ( expected_X , expected_y ) It is a big part of MLOps to test models and data, but this is also heavily reliant on the type of project the team is working and on the type of data they are handling. It is part of the job of the team to define the scope of the tests and choosing the next steps of tests that fulfill the project requirements and try to get the maximum possible coverage at of the code, model behavior and data correctness.","title":"Implementing Tests"},{"location":"old/CICD/tests/#implementing-tests","text":"","title":"Implementing Tests"},{"location":"old/CICD/tests/#why-testing-your-project","text":"In the software development industry it has become a good practice to test your code and try achieving the most possible coverage. This practice helps with making a project that is maintainable, reliable and avoids problems like shipping new versions that break old features. It is important to follow the same practice on the MLOps spectrum by testing the code, the model and even the data. Making tests is somewhat very particular to the project, so this section will show a simple implementation of a testing pipeline using Pytest and Pre-commit, which should later be filled with tests that comprehend the specific project needs.","title":"Why Testing Your Project?"},{"location":"old/CICD/tests/#types-of-testing","text":"It is important to notice that many projects will have different needs in terms of testing, so this division may not be precise for all projects, but in broad terms the main types of testing in MLOps are: Software Testing: Comprehends tests that make sure the code follows the project requirements. This is the type of tests normally implemented in DevOps, such as Unit Testing, Integration Testing, System Testing and Acceptance Testing. Model Testing: Comprehends tests that define that the model is working fine, such as testing that it can be trained, or that it can obtain a minimum score at some evaluation. Data Testing: Comprehends tests that check for data correctness. It is heavily dependant on the project requirements and can be focused on securing that the analyzed dataset follows a schema, contains enough data, and others. Many thing can be tested here depending on the necessities of the team.","title":"Types of testing"},{"location":"old/CICD/tests/#using-pytest","text":"First, we will be using a Python package called Pytest, which is a very popular choice for regular Software testing and can be used to implement all sorts of tests. We are going to build a simple a Pytest file that shows an example of testing the whole project and the team can later add more tests specific for their use cases. In the src/tests/ folder you should see a Python file with a custom name with this content: import pytest def capital_case ( x ): return x . capitalize () def test_capital_case (): assert capital_case ( \"semaphore\" ) == \"Semaphore\" This is a simple file that imports the Pytest package and shows a simple test. capital_case() is a function that could be defined in any place of the entire project, and test_capital_case() is a test function, which means it should be used to test the capital case function. To run all tests: pytest By running pytest on the command line Python finds all the files that have a function starting with the test_ prefix and run them. This means that capital_case() is not run directly and is only running inside test_capital_case() .","title":"Using Pytest"},{"location":"old/CICD/tests/#configuring-pre-commit","text":"Pre-commit is a tool that enables us to use Git Hooks, a particular feature of Git itself, not necessarily Github or GitLab, that enables us to run scripts and commands when performing certain actions such as commit, push, pull, etc. This CLI tool enables us to create a Git Hook easily to test our code before each commit, making it harder for a developer to add code to the Github repository with errors. You should have a .pre-commit-config.yaml file at the root directory with: --- repos : - repo : https://github.com/ambv/black rev : 20.8b1 hooks : - id : black language_version : python3 - repo : local hooks : - id : python-tests name : pytests entry : pytest src/tests language : python additional_dependencies : [ pre-commit , pytest , pandas , sklearn , matplotlib ] always_run : true pass_filenames : false This file can be used to configure a Git Hook that runs Pytest and Black Python formatter before each commit. So, to install this Git Hook use: pre-commit install Now, after you commit some changes in the repository you should see a message stating that all tests passed. If some of the tests didn't pass, the Git Hook won't let you commit until you fix the errors. $ git commit -m \"Example commit\" black....................................................................Passed pytest-check.............................................................Passed","title":"Configuring Pre-commit"},{"location":"old/CICD/tests/#adding-software-testing","text":"Now we are going to write some more tests for the project. One of the most necessary parts of testing in this project is on the Preprocess Pipeline, which comprehends the highest number of functions. So here we will be implementing a test the covers those functions, their ability to read and write files, and that a reduced dataset can be preprocessed with no problems. Create a folder called preprocess/ inside tests/ , and inside this folder a file called test_preprocess.py and another folder called test_data/ , in which we are going to add a simple dataset file testWeatherAUS.csv with the first 2 lines of the real dataset. Now with this created, edit tests/preprocess/test_preprocess.py with some imports and an addition to path to be able to access the test dataset. import io import builtins import pytest import pandas as pd import sys import os # Parent Folder which contains test_data/ sys . path . append ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ )))) ) # Preprocess Python file which contains our functions to be tested import preprocess_data FILE_NAME = \"testWeatherAUS\" DATA_PATH = ( os . path . dirname ( os . path . realpath ( __file__ )) + \"/test_data/\" + FILE_NAME + \".csv\" ) PROCESSED_DATA_PATH = ( os . path . dirname ( os . path . realpath ( __file__ )) + \"/test_data/\" + FILE_NAME + \"_processed.csv\" ) Now, we are going to add unit testing for some of the functions inside preprocess_data.py . def test_count_nulls_by_line (): # Tests function that counts number of nulls by line on a dataframe data = pd . DataFrame ([[ 0 , 2 ], [ 0 , 1 ], [ 6 , None ]]) assert preprocess_data . count_nulls_by_line ( data ) . to_list () == [ 1 , 0 ] def test_null_percent (): # Tests function that gets the percentage of nulls by line on a dataframe data = pd . DataFrame ([[ 0 , 2 ], [ 1 , None ]]) assert preprocess_data . null_percent_by_line ( data ) . to_list () == [ 0.5 , 0 ] Now that we made some examples of unit testing, we should add tests that comprehend the full pipeline, in this case checking if it runs smoothly or returns an error. Pretty general test that can be used to avoid some basic mistakes. This test creates a testWeatherAUS_processed.csv file. This test is going to be important for the next tests, so we will add a mark of dependency, which is a Pytest feature that can be used to tell Python that a test function should be run before another. In this case, any function that marks this function as a dependency will run after this one finishes. @pytest . mark . dependency () def test_preprocess (): # Checks if running the preprocess function returns an error preprocess_data . preprocess_data ( DATA_PATH ) Adding tests that make sure the file is created and is possible to read, which comprehends some system errors. It is possible to see that these functions have marked dependencies, which will make them run after the functions marked. @pytest . mark . dependency ( depends = [ \"test_preprocess\" ]) def test_processed_file_created (): # Checks if the processed file was created during test_preprocess() and is accessible f = open ( PROCESSED_DATA_PATH ) @pytest . mark . dependency ( depends = [ \"test_processed_file_created\" ]) def test_processed_file_format (): # Checks if the processed file is in the correct format (.csv) and can be transformed in dataframe try : pd . read_csv ( PROCESSED_DATA_PATH ) except : raise RuntimeError ( \"Unable to open \" + PROCESSED_DATA_PATH + \" as dataframe\" ) Now that we have implemented our example of tests, we need to delete the file created, which is test_data/testWeatherAUS_processed.csv . A ligature is a feature of Pytest that makes it possible to run code before or after the tests, which are run in the yield call on the code. @pytest . fixture ( scope = \"session\" , autouse = True ) def cleanup ( request ): # Runs tests then cleans up the processed file yield try : os . remove ( PROCESSED_DATA_PATH ) except : pass Now, when you run a pytest , or `git commit which triggers pytest, all these tests should run.","title":"Adding Software Testing"},{"location":"old/CICD/tests/#adding-more-test-files","text":"It is important to note that it is very important to tests the functions that are being used in the scripts of model handling and data handling, such as the function get_variables() in the model.py . Se we are going to follow the same structure and create a file tests/model/test_model.py . In this case we would like to test a large variety of different inputs, so we will be using pytest.mark.parametrize() which enables us to choose a large amount of inputs and expected results. import sys import os import pytest import pandas as pd # Parent Folder sys . path . append ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ )))) ) # Model Python file from model import get_variables FILE_NAME = \"testWeatherAUS\" PROCESSED_DATA_PATH = ( os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ ))) + \"/test_data/\" + FILE_NAME + \"_processed.csv\" ) @pytest . mark . parametrize ( \"expected_X,expected_y\" , [ ( { \"MinTemp\" : { 0 : 13.4 , 1 : 7.4 }, \"MaxTemp\" : { 0 : 22.9 , 1 : 25.1 }, \"Rainfall\" : { 0 : 0.6 , 1 : 0.0 }, \"WindGustSpeed\" : { 0 : 44 , 1 : 44 }, \"WindSpeed9am\" : { 0 : 20 , 1 : 4 }, \"WindSpeed3pm\" : { 0 : 24 , 1 : 22 }, \"Humidity9am\" : { 0 : 71 , 1 : 44 }, \"Humidity3pm\" : { 0 : 22 , 1 : 25 }, \"Pressure9am\" : { 0 : 1007.7 , 1 : 1010.6 }, \"Pressure3pm\" : { 0 : 1007.1 , 1 : 1007.8 }, \"Temp9am\" : { 0 : 16.9 , 1 : 17.2 }, \"Temp3pm\" : { 0 : 21.8 , 1 : 24.3 }, \"RainToday\" : { 0 : 0 , 1 : 0 }, \"WindGustDir_W\" : { 0 : 1 , 1 : 0 }, \"WindGustDir_WNW\" : { 0 : 0 , 1 : 1 }, \"WindDir9am_NNW\" : { 0 : 0 , 1 : 1 }, \"WindDir9am_W\" : { 0 : 1 , 1 : 0 }, \"WindDir3pm_WNW\" : { 0 : 1 , 1 : 0 }, \"WindDir3pm_WSW\" : { 0 : 0 , 1 : 1 }, }, [ 0 , 0 ], ) ], ) def test_get_variables ( expected_X , expected_y ): # Open CSV as DF data = pd . read_csv ( PROCESSED_DATA_PATH ) # Run Function X , y = get_variables ( data , \"RainTomorrow\" ) assert ( X . to_dict (), y . to_list ()) == ( expected_X , expected_y ) It is a big part of MLOps to test models and data, but this is also heavily reliant on the type of project the team is working and on the type of data they are handling. It is part of the job of the team to define the scope of the tests and choosing the next steps of tests that fulfill the project requirements and try to get the maximum possible coverage at of the code, model behavior and data correctness.","title":"Adding More Test Files"},{"location":"old/Deployment/","text":"Deployment with Watson Machine Learning What is Watson Machine Learning? Watson Machine Learning (WML) is a service from the IBM Cloud suite that supports popular frameworks such as TensorFlow, PyTorch, and Keras to build and deploy models. Using this tool we can store, version and deploy models via online deployment. After creating and training a ML model we can upload it as an Asset in the Deployment Space, in the IBM Cloudpak. When we create a new deployment, we choose what model asset we want the deployment to reference: IBM Dataplatform Deocumentation Deployment using Python API To deploy our ML model, we will use IBM's Watson Machine Learning, which will allow us to easily deploy the model as a web service. Since we want to automatize pipelines, we will be creating scripts using the WML Python API . Note The complete script can be found on our example repository The deployment scrip takes the path to the trained model, the path to the root of the project containing the metadata.yaml file, and the credentials file. python3 model_deploy_pipeline . py ./ model_file ../ path / to / project / ../ credentials . yaml Using these arguments constant variables are set import os import sys import yaml MODEL_PATH = os.path.abspath(sys.argv[1]) PROJ_PATH = os.path.abspath(sys.argv[2]) CRED_PATH = os.path.abspath(sys.argv[3]) META_PATH = PROJ_PATH + \"/metadata.yaml\" After that, the yaml files are loaded as dictionaries and the model is loaded (using either joblib or pickle ). with open(CRED_PATH) as stream: try: credentials = yaml.safe_load(stream) except yaml.YAMLError as exc: print(exc) with open(META_PATH) as stream: try: metadata = yaml.safe_load(stream) except yaml.YAMLError as exc: print(exc) with open(MODEL_PATH, \"rb\") as file: # pickle_model = pickle.load(file) pipeline = joblib.load(file) The next step is to create an instance of the IBM Watson client , to do that the credential loaded above will be used and a default Deployment Space will be set using the ID contained in the credentials file, other constants will be set with information\u2019s regarding the model found on the metadata file. from ibm_watson_machine_learning import APIClient wml_credentials = {\"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"]} client = APIClient(wml_credentials) client.spaces.list() MODEL_NAME = metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"] DEPLOY_NAME = MODEL_NAME + \"-Deployment\" MODEL = pipeline SPACE_ID = credentials[\"space_id\"] client.set.default_space(SPACE_ID) Before the deployment, we need to give Watson some characteristics from our model, such as name, type (in this case is scikit-learn_0.23 ) and specifications of the instance that will run the micro-service. Next, the model is stored as an Asset on Watson ML. model_props = { client.repository.ModelMetaNames.NAME: MODEL_NAME, client.repository.ModelMetaNames.TYPE: metadata[\"model_type\"], client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_id_by_name( \"default_py3.7\" ), } model_details = client.repository.store_model(model=MODEL, meta_props=model_props) model_uid = client.repository.get_model_uid(model_details) Once completed, we'll give the deployment a name and then deploy the model using the store ID obtained in the previous step. deployment_props = { client.deployments.ConfigurationMetaNames.NAME: DEPLOY_NAME, client.deployments.ConfigurationMetaNames.ONLINE: {}, } deployment = client.deployments.create( artifact_uid=model_uid, meta_props=deployment_props ) Finally, the storage and deployment IDs are added or updated to in the metadata.yaml file. deployment_uid = client.deployments.get_uid(deployment) metadata[\"model_uid\"] = model_uid metadata[\"deployment_uid\"] = deployment_uid f = open(META_PATH, \"w+\") yaml.dump(metadata, f, allow_unicode=True) Accessing Model Predictions Having deployed the model, we can access it's predictions by sending requests to an end-point or by using the Python ibm_watson_machine_learning library, where we can send either features for a single prediction or payloads containing multiple lines of a dataframe, for example. The payload body is made of the dataframe column names under the \"fields\" key and the features under \"values\" . Watson API payload = { \"input_data\" : [ { \"fields\" : X . columns . to_numpy () . tolist (), \"values\" : X . to_numpy () . tolist (), } ] } result = client . deployments . score ( DEPLOYMENT_UID , payload ) Requests import requests url = \"https://us-south.ml.cloud.ibm.com/ml/v4/deployments?space_id=<string>&tag.value=<string>&asset_id=<string>&version=2020-09-01\" payload = { \"input_data\" : [ { \"fields\" : X . columns . to_numpy () . tolist (), \"values\" : X . to_numpy () . tolist (), } ] } headers = {} response = requests . request ( \"GET\" , url , headers = headers , data = payload ) print ( response . text . encode ( 'utf8' )) The model response will contain the scoring result containing prediction and corresponding probability. In the case of a binary classifier, the response will have the following format: [1, [0.06057910314628456, 0.9394208968537154]], [1, [0.23434887273340754, 0.7656511272665925]], [1, [0.08054183674380211, 0.9194581632561979]], [1, [0.07877206037184215, 0.9212279396281579]], [0, [0.5719774367794239, 0.42802256322057614]], [1, [0.017282880299552716, 0.9827171197004473]], [1, [0.01714869904990468, 0.9828513009500953]], [1, [0.23952044576217457, 0.7604795542378254]], [1, [0.03055527110545664, 0.9694447288945434]], [1, [0.2879899631347379, 0.7120100368652621]], [0, [0.9639766912352016, 0.03602330876479841]], [1, [0.049694416576558154, 0.9503055834234418]], Warning This consumes CUH. Watson Machine Learning CUH are used for running experiments, so there is a limit on how many times you can make requests to the model on a Free Tier. Updating the Model Updating the asset containing the model and/or updating the deployment. Note The complete scripts for the deployment and model can be found on our template repository. Firstly we need to update the model asset in WML by passing the new model as well as a name. print(\"\\nCreating new version\") published_model = client.repository.update_model( model_uid=MODEL_GUID, update_model=model, updated_meta_props={ client.repository.ModelMetaNames.NAME: metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"] }, ) After that a new revision can be created. new_model_revision = client.repository.create_model_revision(MODEL_GUID) rev_id = new_model_revision[\"metadata\"].get(\"rev\") print(\"\\nVersion:\", rev_id) client.repository.list_models_revisions(MODEL_GUID) Finally we can update the deployment. change_meta = {client.deployments.ConfigurationMetaNames.ASSET: {\"id\": MODEL_GUID}} print(\"Updating the following model: \") print(client.deployments.get_details(DEPLOYMENT_UID)) client.deployments.update(DEPLOYMENT_UID, change_meta) Model Rollback We have previously created revisions of a model, to rollback the model version, we'll list all the revisions made. Note Complete script Listing the revisions. client.repository.list_models_revisions(MODEL_GUID) Output: -- ------------- ------------------------ ID NAME CREATED 3 Rain_aus_v0.3 2021-03-31T18:28:07.771Z 2 Rain_aus_v0.3 2021-03-31T18:28:07.771Z 1 Rain_aus_v0.3 2021-03-31T18:28:07.771Z -- ------------- ------------------------ Now we can choose which revision we want to rollback to and then update the deployment referencing that revision ID. MODEL_VERSION = input(\"MODEL VERSION: \") meta = { client.deployments.ConfigurationMetaNames.ASSET: { \"id\": MODEL_GUID, \"rev\": MODEL_VERSION, } } updated_deployment = client.deployments.update( deployment_uid=DEPLOYMENT_UID, changes=meta ) Finally, we'll wait for the update to finish so we can see if it was successful. status = None while status not in [\"ready\", \"failed\"]: print(\".\", end=\" \") time.sleep(2) deployment_details = client.deployments.get_details(DEPLOYMENT_UID) status = deployment_details[\"entity\"][\"status\"].get(\"state\") print(\"\\nDeployment update finished with status: \", status)","title":"Deployment with Watson Machine Learning"},{"location":"old/Deployment/#deployment-with-watson-machine-learning","text":"","title":"Deployment with Watson Machine Learning"},{"location":"old/Deployment/#what-is-watson-machine-learning","text":"Watson Machine Learning (WML) is a service from the IBM Cloud suite that supports popular frameworks such as TensorFlow, PyTorch, and Keras to build and deploy models. Using this tool we can store, version and deploy models via online deployment. After creating and training a ML model we can upload it as an Asset in the Deployment Space, in the IBM Cloudpak. When we create a new deployment, we choose what model asset we want the deployment to reference: IBM Dataplatform Deocumentation","title":"What is Watson Machine Learning?"},{"location":"old/Deployment/#deployment-using-python-api","text":"To deploy our ML model, we will use IBM's Watson Machine Learning, which will allow us to easily deploy the model as a web service. Since we want to automatize pipelines, we will be creating scripts using the WML Python API . Note The complete script can be found on our example repository The deployment scrip takes the path to the trained model, the path to the root of the project containing the metadata.yaml file, and the credentials file. python3 model_deploy_pipeline . py ./ model_file ../ path / to / project / ../ credentials . yaml Using these arguments constant variables are set import os import sys import yaml MODEL_PATH = os.path.abspath(sys.argv[1]) PROJ_PATH = os.path.abspath(sys.argv[2]) CRED_PATH = os.path.abspath(sys.argv[3]) META_PATH = PROJ_PATH + \"/metadata.yaml\" After that, the yaml files are loaded as dictionaries and the model is loaded (using either joblib or pickle ). with open(CRED_PATH) as stream: try: credentials = yaml.safe_load(stream) except yaml.YAMLError as exc: print(exc) with open(META_PATH) as stream: try: metadata = yaml.safe_load(stream) except yaml.YAMLError as exc: print(exc) with open(MODEL_PATH, \"rb\") as file: # pickle_model = pickle.load(file) pipeline = joblib.load(file) The next step is to create an instance of the IBM Watson client , to do that the credential loaded above will be used and a default Deployment Space will be set using the ID contained in the credentials file, other constants will be set with information\u2019s regarding the model found on the metadata file. from ibm_watson_machine_learning import APIClient wml_credentials = {\"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"]} client = APIClient(wml_credentials) client.spaces.list() MODEL_NAME = metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"] DEPLOY_NAME = MODEL_NAME + \"-Deployment\" MODEL = pipeline SPACE_ID = credentials[\"space_id\"] client.set.default_space(SPACE_ID) Before the deployment, we need to give Watson some characteristics from our model, such as name, type (in this case is scikit-learn_0.23 ) and specifications of the instance that will run the micro-service. Next, the model is stored as an Asset on Watson ML. model_props = { client.repository.ModelMetaNames.NAME: MODEL_NAME, client.repository.ModelMetaNames.TYPE: metadata[\"model_type\"], client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_id_by_name( \"default_py3.7\" ), } model_details = client.repository.store_model(model=MODEL, meta_props=model_props) model_uid = client.repository.get_model_uid(model_details) Once completed, we'll give the deployment a name and then deploy the model using the store ID obtained in the previous step. deployment_props = { client.deployments.ConfigurationMetaNames.NAME: DEPLOY_NAME, client.deployments.ConfigurationMetaNames.ONLINE: {}, } deployment = client.deployments.create( artifact_uid=model_uid, meta_props=deployment_props ) Finally, the storage and deployment IDs are added or updated to in the metadata.yaml file. deployment_uid = client.deployments.get_uid(deployment) metadata[\"model_uid\"] = model_uid metadata[\"deployment_uid\"] = deployment_uid f = open(META_PATH, \"w+\") yaml.dump(metadata, f, allow_unicode=True)","title":"Deployment using Python API"},{"location":"old/Deployment/#accessing-model-predictions","text":"Having deployed the model, we can access it's predictions by sending requests to an end-point or by using the Python ibm_watson_machine_learning library, where we can send either features for a single prediction or payloads containing multiple lines of a dataframe, for example. The payload body is made of the dataframe column names under the \"fields\" key and the features under \"values\" . Watson API payload = { \"input_data\" : [ { \"fields\" : X . columns . to_numpy () . tolist (), \"values\" : X . to_numpy () . tolist (), } ] } result = client . deployments . score ( DEPLOYMENT_UID , payload ) Requests import requests url = \"https://us-south.ml.cloud.ibm.com/ml/v4/deployments?space_id=<string>&tag.value=<string>&asset_id=<string>&version=2020-09-01\" payload = { \"input_data\" : [ { \"fields\" : X . columns . to_numpy () . tolist (), \"values\" : X . to_numpy () . tolist (), } ] } headers = {} response = requests . request ( \"GET\" , url , headers = headers , data = payload ) print ( response . text . encode ( 'utf8' )) The model response will contain the scoring result containing prediction and corresponding probability. In the case of a binary classifier, the response will have the following format: [1, [0.06057910314628456, 0.9394208968537154]], [1, [0.23434887273340754, 0.7656511272665925]], [1, [0.08054183674380211, 0.9194581632561979]], [1, [0.07877206037184215, 0.9212279396281579]], [0, [0.5719774367794239, 0.42802256322057614]], [1, [0.017282880299552716, 0.9827171197004473]], [1, [0.01714869904990468, 0.9828513009500953]], [1, [0.23952044576217457, 0.7604795542378254]], [1, [0.03055527110545664, 0.9694447288945434]], [1, [0.2879899631347379, 0.7120100368652621]], [0, [0.9639766912352016, 0.03602330876479841]], [1, [0.049694416576558154, 0.9503055834234418]], Warning This consumes CUH. Watson Machine Learning CUH are used for running experiments, so there is a limit on how many times you can make requests to the model on a Free Tier.","title":"Accessing Model Predictions"},{"location":"old/Deployment/#updating-the-model","text":"Updating the asset containing the model and/or updating the deployment. Note The complete scripts for the deployment and model can be found on our template repository. Firstly we need to update the model asset in WML by passing the new model as well as a name. print(\"\\nCreating new version\") published_model = client.repository.update_model( model_uid=MODEL_GUID, update_model=model, updated_meta_props={ client.repository.ModelMetaNames.NAME: metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"] }, ) After that a new revision can be created. new_model_revision = client.repository.create_model_revision(MODEL_GUID) rev_id = new_model_revision[\"metadata\"].get(\"rev\") print(\"\\nVersion:\", rev_id) client.repository.list_models_revisions(MODEL_GUID) Finally we can update the deployment. change_meta = {client.deployments.ConfigurationMetaNames.ASSET: {\"id\": MODEL_GUID}} print(\"Updating the following model: \") print(client.deployments.get_details(DEPLOYMENT_UID)) client.deployments.update(DEPLOYMENT_UID, change_meta)","title":"Updating the Model"},{"location":"old/Deployment/#model-rollback","text":"We have previously created revisions of a model, to rollback the model version, we'll list all the revisions made. Note Complete script Listing the revisions. client.repository.list_models_revisions(MODEL_GUID) Output: -- ------------- ------------------------ ID NAME CREATED 3 Rain_aus_v0.3 2021-03-31T18:28:07.771Z 2 Rain_aus_v0.3 2021-03-31T18:28:07.771Z 1 Rain_aus_v0.3 2021-03-31T18:28:07.771Z -- ------------- ------------------------ Now we can choose which revision we want to rollback to and then update the deployment referencing that revision ID. MODEL_VERSION = input(\"MODEL VERSION: \") meta = { client.deployments.ConfigurationMetaNames.ASSET: { \"id\": MODEL_GUID, \"rev\": MODEL_VERSION, } } updated_deployment = client.deployments.update( deployment_uid=DEPLOYMENT_UID, changes=meta ) Finally, we'll wait for the update to finish so we can see if it was successful. status = None while status not in [\"ready\", \"failed\"]: print(\".\", end=\" \") time.sleep(2) deployment_details = client.deployments.get_details(DEPLOYMENT_UID) status = deployment_details[\"entity\"][\"status\"].get(\"state\") print(\"\\nDeployment update finished with status: \", status)","title":"Model Rollback"},{"location":"old/Infraestrutura/Auth/","text":"IAM Access To manage users' access to the infrastructure resources is a good practice both as a server health safety and a cybersecurity matter, as it makes harder to gain admin access to those resources. In this section, it will be shown how to create your own API Key to use in many steps on this tutorial. Steps Login in your IBM cloud account and go to your dashboard. Click on the 'Manage' tab located in the top bar. On the left menu, go to 'API keys'. Finally, create your API key and keep it safe. Warning If by any means your API key leaks, it is very import to create a new one. Just follow the same steps above, but delete your old key. Notes For more information about account security access: What is IBM Cloud Identity and Access Management?","title":"IAM Access"},{"location":"old/Infraestrutura/Auth/#iam-access","text":"To manage users' access to the infrastructure resources is a good practice both as a server health safety and a cybersecurity matter, as it makes harder to gain admin access to those resources. In this section, it will be shown how to create your own API Key to use in many steps on this tutorial.","title":"IAM Access"},{"location":"old/Infraestrutura/Auth/#steps","text":"Login in your IBM cloud account and go to your dashboard. Click on the 'Manage' tab located in the top bar. On the left menu, go to 'API keys'. Finally, create your API key and keep it safe. Warning If by any means your API key leaks, it is very import to create a new one. Just follow the same steps above, but delete your old key. Notes For more information about account security access: What is IBM Cloud Identity and Access Management?","title":"Steps"},{"location":"old/Infraestrutura/Python/","text":"Managing the deployment space The current IBM's terraform module is in development and some features are still missing. So we will use a Python script to manage the IBM Watson's deployment space, including create, delete and get the space_id , which will be very important as it is used in other scripts. Authentication The following code is used to authenticate to the provider, note that it uses the same environment variable as terraform. import os import sys from pprint import pprint import json from ibm_watson_machine_learning import APIClient TERRAFORM_OUTPUT = '.terraform/terraform.tfstate' def authentication(): if os.getenv(\"IBMCLOUD_API_KEY\"): wml_credentials = { \"url\": \"https://us-south.ml.cloud.ibm.com\", \"apikey\": os.environ.get(\"IBMCLOUD_API_KEY\"), } client = APIClient(wml_credentials) # Connect to IBM cloud return client raise Exception(\"API_KEY environment variable not defined\") Terraform output To create a deployment space, we need to get some metadata from the resources created by the terraform script. Here we use the output defined on terraform. def terraform_output(terraform_path=TERRAFORM_OUTPUT): output = dict(json.load(open(terraform_path)))['outputs'] cos_crn = output[\"cos_crn\"][\"value\"] wml_crn = output[\"wml_crn\"][\"value\"][\"crn\"] wml_name = output[\"wml_crn\"][\"value\"][\"resource_name\"] state = { \"cos_crn\" : cos_crn, \"wml_name\": wml_name, \"wml_crn\" : wml_crn } return state Creating a space Now with the metadata in hands, we can finally create a deployment space. def create_deployment_space( client, cos_crn, wml_name, wml_crn, space_name=\"default\", description=\"\" ): ## Project info metadata = { client.spaces.ConfigurationMetaNames.NAME: space_name, client.spaces.ConfigurationMetaNames.DESCRIPTION: description, client.spaces.ConfigurationMetaNames.STORAGE: { \"type\": \"bmcos_object_storage\", \"resource_crn\": cos_crn, }, ## Project compute instance (WML) client.spaces.ConfigurationMetaNames.COMPUTE: { \"name\": wml_name, \"crn\": wml_crn, }, } space_details = client.spaces.store(meta_props=metadata) # Create a space return space_details Get the space_id With the space created, now we have the space_id , the following function is used to retrieve it. This info will be used on other scripts that will be shown on next pages. def update_deployment_space(client, new_name, space_id): metadata = {client.spaces.ConfigurationMetaNames.NAME: new_name} space_details = client.spaces.update(space_id, changes=metadata) return space_details Note To see the complete script click here","title":"Managing the deployment space"},{"location":"old/Infraestrutura/Python/#managing-the-deployment-space","text":"The current IBM's terraform module is in development and some features are still missing. So we will use a Python script to manage the IBM Watson's deployment space, including create, delete and get the space_id , which will be very important as it is used in other scripts.","title":"Managing the deployment space"},{"location":"old/Infraestrutura/Python/#authentication","text":"The following code is used to authenticate to the provider, note that it uses the same environment variable as terraform. import os import sys from pprint import pprint import json from ibm_watson_machine_learning import APIClient TERRAFORM_OUTPUT = '.terraform/terraform.tfstate' def authentication(): if os.getenv(\"IBMCLOUD_API_KEY\"): wml_credentials = { \"url\": \"https://us-south.ml.cloud.ibm.com\", \"apikey\": os.environ.get(\"IBMCLOUD_API_KEY\"), } client = APIClient(wml_credentials) # Connect to IBM cloud return client raise Exception(\"API_KEY environment variable not defined\")","title":"Authentication"},{"location":"old/Infraestrutura/Python/#terraform-output","text":"To create a deployment space, we need to get some metadata from the resources created by the terraform script. Here we use the output defined on terraform. def terraform_output(terraform_path=TERRAFORM_OUTPUT): output = dict(json.load(open(terraform_path)))['outputs'] cos_crn = output[\"cos_crn\"][\"value\"] wml_crn = output[\"wml_crn\"][\"value\"][\"crn\"] wml_name = output[\"wml_crn\"][\"value\"][\"resource_name\"] state = { \"cos_crn\" : cos_crn, \"wml_name\": wml_name, \"wml_crn\" : wml_crn } return state","title":"Terraform output"},{"location":"old/Infraestrutura/Python/#creating-a-space","text":"Now with the metadata in hands, we can finally create a deployment space. def create_deployment_space( client, cos_crn, wml_name, wml_crn, space_name=\"default\", description=\"\" ): ## Project info metadata = { client.spaces.ConfigurationMetaNames.NAME: space_name, client.spaces.ConfigurationMetaNames.DESCRIPTION: description, client.spaces.ConfigurationMetaNames.STORAGE: { \"type\": \"bmcos_object_storage\", \"resource_crn\": cos_crn, }, ## Project compute instance (WML) client.spaces.ConfigurationMetaNames.COMPUTE: { \"name\": wml_name, \"crn\": wml_crn, }, } space_details = client.spaces.store(meta_props=metadata) # Create a space return space_details","title":"Creating a space"},{"location":"old/Infraestrutura/Python/#get-the-space_id","text":"With the space created, now we have the space_id , the following function is used to retrieve it. This info will be used on other scripts that will be shown on next pages. def update_deployment_space(client, new_name, space_id): metadata = {client.spaces.ConfigurationMetaNames.NAME: new_name} space_details = client.spaces.update(space_id, changes=metadata) return space_details Note To see the complete script click here","title":"Get the space_id"},{"location":"old/Infraestrutura/Terraform/","text":"Setting Up the IBM Environment with Terraform Introduction Infrastructure as a code(IaC) is a process of managing and provisioning mechanisms for authenticating, planning and implementing servers and data centers in the cloud and private network without the need to manually configure everything through the provider's UI. IaC is a good practice that has been gaining attention since the DevOps culture started to grow and it is one of many processes that MLOps shares with it. For example, it makes easier to fast redeploy the infrastructure in a disaster scenario or an alternative plan to quickly change providers. For this example, we will use Terraform to deploy the needed IBM resources and a Python script to manage the deployment space inside the IBM Watson environment. What is Terraform? Terraform is an open-source infrastructure as a code software tool created by HashiCorp. It enables users to define and provision an infrastructure in a high-level configuration language. It saves the current state and any changes made on the script will only make the changes needed, e.g., change an instance name won't reset the instance itself or make any changes on other resources. Requirements We need to install the terraform and the IBM's module. Terraform Script # IBM's module. # It will download the module when run \"terraform init\" terraform { required_providers { ibm = { source = \"IBM-Cloud/ibm\" version = \"~> 1.12.0\" } } } provider \"ibm\" {} # This will create a resource group # It separates the resources used inside IBM's cloud data \"ibm_resource_group\" \"group\" { name = \"GROUP_NAME\" } # This part will deploy a Watson machine learning resource resource \"ibm_resource_instance\" \"wml\" { name = \"WML_NAME\" service = \"pm-20\" plan = \"lite\" location = \"us-south\" resource_group_id = data.ibm_resource_group.group.id tags = [\"TEST\", \"TERRAFORM\"] } # This deploys a IBM Cloud Object Storage resource resource \"ibm_resource_instance\" \"cos\" { name = \"COS_NAME\" service = \"cloud-object-storage\" plan = \"standard\" location = \"global\" resource_group_id = data.ibm_resource_group.group.id tags = [\"TERRAFORM\", \"TEST\"] } This script will create: Work group Watson Machine Learning resource IBM COS instance To install IBM's module run terraform init . Authentication Before you continue, you will need to create an API Key and assign it to an environment variable called IBMCLOUD_API_KEY . Terraform Plan and Apply If we run the command terraform plan the following output shows all the changes that will be made, in this case, create all the resources. Terraform will perform the following actions: # ibm_resource_instance.cos will be created + resource \"ibm_resource_instance\" \"cos\" { + crn = (known after apply) + dashboard_url = (known after apply) + extensions = (known after apply) + guid = (known after apply) + id = (known after apply) + location = \"global\" + name = \"TESTE_COS\" + plan = \"standard\" + resource_controller_url = (known after apply) + resource_crn = (known after apply) + resource_group_id = \"3f5502dbe92e42579cfdfec471f3ebd5\" + resource_group_name = (known after apply) + resource_name = (known after apply) + resource_status = (known after apply) + service = \"cloud-object-storage\" + status = (known after apply) + tags = [ + \"TERRAFORM\", + \"TEST\", ] } # ibm_resource_instance.wml will be created + resource \"ibm_resource_instance\" \"wml\" { + crn = (known after apply) + dashboard_url = (known after apply) + extensions = (known after apply) + guid = (known after apply) + id = (known after apply) + location = \"us-south\" + name = \"TESTE_TERRAFORM\" + plan = \"lite\" + resource_controller_url = (known after apply) + resource_crn = (known after apply) + resource_group_id = \"3f5502dbe92e42579cfdfec471f3ebd5\" + resource_group_name = (known after apply) + resource_name = (known after apply) + resource_status = (known after apply) + service = \"pm-20\" + status = (known after apply) + tags = [ + \"TERRAFORM\", + \"TESTE\", ] } Plan: 2 to add, 0 to change, 0 to destroy. If everything is correct, then run terraform apply to create the infrastructure. Outputs After everything is set, terraform create a state file with the resources metadata. This file can be quite overwhelming to work with and we will need some of this metadata. That's the use for an output file, when we run terraform apply , it detects this file and create a section on the state file with the information needed and it is easier to work with. The following script is an output file with the metadata that will be used on the next steps. output \"cos_crn\" { value = ibm_resource_instance.cos.crn } output \"wml_name\" { value = ibm_resource_instance.wml.name } output \"wml_crn\" { value = ibm_resource_instance.wml } It is very straightforward and easy to read and understand. Note that, it is possible to create this file after everything is deployed and run a terraform apply again without worrying about the instances.","title":"Setting Up the IBM Environment with Terraform"},{"location":"old/Infraestrutura/Terraform/#setting-up-the-ibm-environment-with-terraform","text":"","title":"Setting Up the IBM Environment with Terraform"},{"location":"old/Infraestrutura/Terraform/#introduction","text":"Infrastructure as a code(IaC) is a process of managing and provisioning mechanisms for authenticating, planning and implementing servers and data centers in the cloud and private network without the need to manually configure everything through the provider's UI. IaC is a good practice that has been gaining attention since the DevOps culture started to grow and it is one of many processes that MLOps shares with it. For example, it makes easier to fast redeploy the infrastructure in a disaster scenario or an alternative plan to quickly change providers. For this example, we will use Terraform to deploy the needed IBM resources and a Python script to manage the deployment space inside the IBM Watson environment.","title":"Introduction"},{"location":"old/Infraestrutura/Terraform/#what-is-terraform","text":"Terraform is an open-source infrastructure as a code software tool created by HashiCorp. It enables users to define and provision an infrastructure in a high-level configuration language. It saves the current state and any changes made on the script will only make the changes needed, e.g., change an instance name won't reset the instance itself or make any changes on other resources.","title":"What is Terraform?"},{"location":"old/Infraestrutura/Terraform/#requirements","text":"We need to install the terraform and the IBM's module.","title":"Requirements"},{"location":"old/Infraestrutura/Terraform/#terraform-script","text":"# IBM's module. # It will download the module when run \"terraform init\" terraform { required_providers { ibm = { source = \"IBM-Cloud/ibm\" version = \"~> 1.12.0\" } } } provider \"ibm\" {} # This will create a resource group # It separates the resources used inside IBM's cloud data \"ibm_resource_group\" \"group\" { name = \"GROUP_NAME\" } # This part will deploy a Watson machine learning resource resource \"ibm_resource_instance\" \"wml\" { name = \"WML_NAME\" service = \"pm-20\" plan = \"lite\" location = \"us-south\" resource_group_id = data.ibm_resource_group.group.id tags = [\"TEST\", \"TERRAFORM\"] } # This deploys a IBM Cloud Object Storage resource resource \"ibm_resource_instance\" \"cos\" { name = \"COS_NAME\" service = \"cloud-object-storage\" plan = \"standard\" location = \"global\" resource_group_id = data.ibm_resource_group.group.id tags = [\"TERRAFORM\", \"TEST\"] } This script will create: Work group Watson Machine Learning resource IBM COS instance To install IBM's module run terraform init .","title":"Terraform Script"},{"location":"old/Infraestrutura/Terraform/#authentication","text":"Before you continue, you will need to create an API Key and assign it to an environment variable called IBMCLOUD_API_KEY .","title":"Authentication"},{"location":"old/Infraestrutura/Terraform/#terraform-plan-and-apply","text":"If we run the command terraform plan the following output shows all the changes that will be made, in this case, create all the resources. Terraform will perform the following actions: # ibm_resource_instance.cos will be created + resource \"ibm_resource_instance\" \"cos\" { + crn = (known after apply) + dashboard_url = (known after apply) + extensions = (known after apply) + guid = (known after apply) + id = (known after apply) + location = \"global\" + name = \"TESTE_COS\" + plan = \"standard\" + resource_controller_url = (known after apply) + resource_crn = (known after apply) + resource_group_id = \"3f5502dbe92e42579cfdfec471f3ebd5\" + resource_group_name = (known after apply) + resource_name = (known after apply) + resource_status = (known after apply) + service = \"cloud-object-storage\" + status = (known after apply) + tags = [ + \"TERRAFORM\", + \"TEST\", ] } # ibm_resource_instance.wml will be created + resource \"ibm_resource_instance\" \"wml\" { + crn = (known after apply) + dashboard_url = (known after apply) + extensions = (known after apply) + guid = (known after apply) + id = (known after apply) + location = \"us-south\" + name = \"TESTE_TERRAFORM\" + plan = \"lite\" + resource_controller_url = (known after apply) + resource_crn = (known after apply) + resource_group_id = \"3f5502dbe92e42579cfdfec471f3ebd5\" + resource_group_name = (known after apply) + resource_name = (known after apply) + resource_status = (known after apply) + service = \"pm-20\" + status = (known after apply) + tags = [ + \"TERRAFORM\", + \"TESTE\", ] } Plan: 2 to add, 0 to change, 0 to destroy. If everything is correct, then run terraform apply to create the infrastructure.","title":"Terraform Plan and Apply"},{"location":"old/Infraestrutura/Terraform/#outputs","text":"After everything is set, terraform create a state file with the resources metadata. This file can be quite overwhelming to work with and we will need some of this metadata. That's the use for an output file, when we run terraform apply , it detects this file and create a section on the state file with the information needed and it is easier to work with. The following script is an output file with the metadata that will be used on the next steps. output \"cos_crn\" { value = ibm_resource_instance.cos.crn } output \"wml_name\" { value = ibm_resource_instance.wml.name } output \"wml_crn\" { value = ibm_resource_instance.wml } It is very straightforward and easy to read and understand. Note that, it is possible to create this file after everything is deployed and run a terraform apply again without worrying about the instances.","title":"Outputs"},{"location":"old/MLOps/CICDML/","text":"CI/CD for Machine Learning Just like in DevOps, CI/CD is a method to make changes more frequently by automating the development stages. In machine learning(ML) this stages are different than a software development, a model depends not only on the code but also the data and hyperparameters, as well as deploying a model to production is more complex too. Continuous Integration (CI) Continuous integration in ML means that every time a code or data is updated the ML pipeline reruns, this is done in a way that everything is versioned and reproducible, so it is possible to share the codebase across projects and teams. Every rerun may consist in training, testing or generating new reports, making easier to compare between other versions in production. Note that, it is possible and recommended to run code tests too, for example, ensuring the code is in certain format, dataset values, such as NaN or wrong data types or functions outputs. Some examples of a CI workflow: running and versioning the training and evaluation for every commit to the repository. running and comparing experiment runs for each Pull Request to a certain branch. trigger a new run periodically. Continuous Deployment (CD) Continuous deployment is a method to automate the deployment of the new release to production, or any environment such as staging. This practice makes it easier to receive users' feedback, as the changes are faster and constant, as well as new data for retraining or new models. Some examples of CD workflow: Verify the requirements on the infrastructure environment before deploying it. Test the model output based on a known input. Load testing and model latency. Popular CI/CD tools for Machine Learning There still aren't many alternatives for CI/CD in regards of Machine Learning. Regular CI/CD can be used if correctly implemented, such as running tests, deploy scripts, etc. focused on the MLOps environment. Although there are some tools preferred for most projects. Tools License Developer Observations CML (Continuous Machine Learning) Open-source Iterative Most popular tools for CI/CD specific for Machine Learning. By the same developers of DVC, it can be integrated into it. Can be easily used with Github Actions or Gitlab CI/CD. Jenkins Open-source Jenkins CI Jenkins is a popular tool for regular CI/CD that can be used for Machine Learning after some configuration. It is a popular choice among some MLOps projects that intend to run tests on local hardware or heavily configured cloud services. Image from Cloud HM blog. https://blog.cloudhm.co.th/ci-cd/ \u21a9","title":"CI/CD for Machine Learning"},{"location":"old/MLOps/CICDML/#cicd-for-machine-learning","text":"Just like in DevOps, CI/CD is a method to make changes more frequently by automating the development stages. In machine learning(ML) this stages are different than a software development, a model depends not only on the code but also the data and hyperparameters, as well as deploying a model to production is more complex too.","title":"CI/CD for Machine Learning"},{"location":"old/MLOps/CICDML/#continuous-integration-ci","text":"Continuous integration in ML means that every time a code or data is updated the ML pipeline reruns, this is done in a way that everything is versioned and reproducible, so it is possible to share the codebase across projects and teams. Every rerun may consist in training, testing or generating new reports, making easier to compare between other versions in production. Note that, it is possible and recommended to run code tests too, for example, ensuring the code is in certain format, dataset values, such as NaN or wrong data types or functions outputs. Some examples of a CI workflow: running and versioning the training and evaluation for every commit to the repository. running and comparing experiment runs for each Pull Request to a certain branch. trigger a new run periodically.","title":"Continuous Integration (CI)"},{"location":"old/MLOps/CICDML/#continuous-deployment-cd","text":"Continuous deployment is a method to automate the deployment of the new release to production, or any environment such as staging. This practice makes it easier to receive users' feedback, as the changes are faster and constant, as well as new data for retraining or new models. Some examples of CD workflow: Verify the requirements on the infrastructure environment before deploying it. Test the model output based on a known input. Load testing and model latency.","title":"Continuous Deployment (CD)"},{"location":"old/MLOps/CICDML/#popular-cicd-tools-for-machine-learning","text":"There still aren't many alternatives for CI/CD in regards of Machine Learning. Regular CI/CD can be used if correctly implemented, such as running tests, deploy scripts, etc. focused on the MLOps environment. Although there are some tools preferred for most projects. Tools License Developer Observations CML (Continuous Machine Learning) Open-source Iterative Most popular tools for CI/CD specific for Machine Learning. By the same developers of DVC, it can be integrated into it. Can be easily used with Github Actions or Gitlab CI/CD. Jenkins Open-source Jenkins CI Jenkins is a popular tool for regular CI/CD that can be used for Machine Learning after some configuration. It is a popular choice among some MLOps projects that intend to run tests on local hardware or heavily configured cloud services. Image from Cloud HM blog. https://blog.cloudhm.co.th/ci-cd/ \u21a9","title":"Popular CI/CD tools for Machine Learning"},{"location":"old/MLOps/Data/","text":"Versioning Data and Model Versioning The use of code versioning tools is vital in the software development industry. The possibility of replicating the same code base so that several people can work on the same project simultaneously is a great benefit. In addition, versioning these bases allows them to work in different sections in an organized manner and without compromising the integrity of the code in production. As much as these tools solve several problems in software development, there are still issues in machine learning projects. Code versioning is still crucial, but when working on new experiments it's important to guarantee the same properties for data and models. In a machine learning project, data scientists are continuously working on the development of new models. This process relies on trying different combinations of data, parameters, and algorithms. It's extremely positive to create an environment where it's possible to go back and forth on older or new experiments. Reproducibility When discussing versioning, it's important to understand the term reproducibility . While versioning data, models, and code we are able to create a nice environment for data scientists to achieve the ultimate goal that is a good working model, there is a huge gap between this positive experiment to operationalize it. To guarantee that the experimentation of the data science team will become a model in the production for the project, it's important to make sure that key factors are documented and reusable. The following factors listed below were extracted from \"Introducing MLOps\" (Treveil and Dataiku Team 57) : Assumptions: Data Scientist's decisions and assumptions must be explicit. Randomness: Considering that some machine learning experiments contain pseudo-randomness, this needs to be in some kind of control so it can be reproduced. For example, using \"seed\". Data: The same data of the experiment must be available. Settings: Repeat and reproduce experiments with the same settings from the original. Implementation: Especially with complex models, different implementations can have different results. This is important to keep in mind when debugging. Environment: It's crucial to have the same runtime configurations among all data scientists. Popular Versioning Tools Merging data, model, and code versioning isn't an easy task but fortunately, there are several tools being created or constantly updated to feel the need for data and model versioning. These tools can offer ways for transforming data with pipelines to configuring models. We listed below some of these tools available for usage: Tool License Developer Observations IBM Watson ML Proprietary IBM Focused on model versioning DVC Open-source Iterative Popular lightweight open-source tool focused on data, model and pipeline versioning. Can be easily integrated with CML. Pachyderm Open-source Pachyderm Data platform built on Docker and Kubernetes. Focused on Version Control and automating workloads with parallelization. MLflow Open-source Databricks Popular tool for many parts of the Machine Learning lifecycle, including versioning of processes and experiments. Can be easily integrated with other tools Git LFS (Large File System) Open-source Atlassian, GitHub, and others Git extension that permits large files on Git repositories. Can be used to share large data files and models, using Git versioning. Image from DVC Documentation. https://dvc.org/doc/use-cases/versioning-data-and-model-files \u21a9","title":"Versioning"},{"location":"old/MLOps/Data/#versioning","text":"","title":"Versioning"},{"location":"old/MLOps/Data/#data-and-model-versioning","text":"The use of code versioning tools is vital in the software development industry. The possibility of replicating the same code base so that several people can work on the same project simultaneously is a great benefit. In addition, versioning these bases allows them to work in different sections in an organized manner and without compromising the integrity of the code in production. As much as these tools solve several problems in software development, there are still issues in machine learning projects. Code versioning is still crucial, but when working on new experiments it's important to guarantee the same properties for data and models. In a machine learning project, data scientists are continuously working on the development of new models. This process relies on trying different combinations of data, parameters, and algorithms. It's extremely positive to create an environment where it's possible to go back and forth on older or new experiments.","title":"Data and Model Versioning"},{"location":"old/MLOps/Data/#reproducibility","text":"When discussing versioning, it's important to understand the term reproducibility . While versioning data, models, and code we are able to create a nice environment for data scientists to achieve the ultimate goal that is a good working model, there is a huge gap between this positive experiment to operationalize it. To guarantee that the experimentation of the data science team will become a model in the production for the project, it's important to make sure that key factors are documented and reusable. The following factors listed below were extracted from \"Introducing MLOps\" (Treveil and Dataiku Team 57) : Assumptions: Data Scientist's decisions and assumptions must be explicit. Randomness: Considering that some machine learning experiments contain pseudo-randomness, this needs to be in some kind of control so it can be reproduced. For example, using \"seed\". Data: The same data of the experiment must be available. Settings: Repeat and reproduce experiments with the same settings from the original. Implementation: Especially with complex models, different implementations can have different results. This is important to keep in mind when debugging. Environment: It's crucial to have the same runtime configurations among all data scientists.","title":"Reproducibility"},{"location":"old/MLOps/Data/#popular-versioning-tools","text":"Merging data, model, and code versioning isn't an easy task but fortunately, there are several tools being created or constantly updated to feel the need for data and model versioning. These tools can offer ways for transforming data with pipelines to configuring models. We listed below some of these tools available for usage: Tool License Developer Observations IBM Watson ML Proprietary IBM Focused on model versioning DVC Open-source Iterative Popular lightweight open-source tool focused on data, model and pipeline versioning. Can be easily integrated with CML. Pachyderm Open-source Pachyderm Data platform built on Docker and Kubernetes. Focused on Version Control and automating workloads with parallelization. MLflow Open-source Databricks Popular tool for many parts of the Machine Learning lifecycle, including versioning of processes and experiments. Can be easily integrated with other tools Git LFS (Large File System) Open-source Atlassian, GitHub, and others Git extension that permits large files on Git repositories. Can be used to share large data files and models, using Git versioning. Image from DVC Documentation. https://dvc.org/doc/use-cases/versioning-data-and-model-files \u21a9","title":"Popular Versioning Tools"},{"location":"old/MLOps/FeatureStore/","text":"Feature Storing What is a Feature Store? Feature Stores are components of data architecture that are becoming increasingly popular in the Machine Learning and MLOps environment. The goal of a Feature Store is to process data from various data sources at the same time and turn it into features, which will be consumed by the model training pipeline and the model serving. The concept of Feature Stores is novice and rapidly changing, therefore this page has the objective of showing the key features that are more common among the main Feature Stores in the market, but at the same time it is important to note that some of the tools and frameworks in the market might not comprehend all those exact characteristics in the same manner. Why it matters? Feature Stores can be very useful for Machine Learning in production and are very reliable ways to manage features for research and training using Offline Stores, as it is to manage the feeding of features to a model served in production using an Online Store. This data component can manage to comprehend a wide spectrum of different projects and necessities, some of which are seen below. Key Features Enables features to be shared by multiple teams of Data Scientists working at the same time. Creates a reliable automated preprocess pipeline of large quantities of data. Can use and combine different data sources, such as data lakes, data warehouses and streaming of new data, all at once. Provides relevant and online features to a model in production. Can use a time windows system for Data Scientists to gather features from any point in time. Highly customizable for different model needs of consumption, such as batch or real-time predictions. Offline Store vs Online Store Feature Stores combine multiple data sources and preprocess those into features, the main types of data are: Batch Data : Usually coming from Data Lakes or Data Warehouses. Those are big chunks of data that have been stored in order to be used by models and are not necessarily updated in real-time. Example: Data from customers of a bank, such as age, country, etc. Real-time Data : Usually coming from Streaming and Log events. Those the online data that are constantly coming from sources like the events logged on a system. Example: A transaction in a bank is logged in real-time and fed to the Feature Store. Those types of data are combined inside and form two types of stores: Offline Stores: Store composed of preprocessed features of Batch Data, used for building a historical source of features, that can be used by Data Scientists in the Model Training pipeline. With it's historical components, in most Feature Stores it can be used to provide a series of features at a given time frame or time point. It is normally stored in data warehouses, like IBM Cloud Object Storage, Apache Hive or S3, or in databases, like PostgreSQL, Cassandra and MySQL, but it can also be used in other kinds of systems, like HDFS. Online Stores: Store composed of data from the Offline Store combined with real-time preprocessed features from streaming data sources. It is built with the objective of being the most up-to-date collection of organized features, which can be used to feed the Model in Production with new features for prediction. It is normally stored in databases for rapid access, like MySQL, Cassandra, Redis, but it can be stored in more complex systems. Common Architecture Popular Feature Stores Many companies relay on proprietary software to develop Feature Stores, since many try to make a component that fits into their exactly use-case, such a as: Tools License Developer Observations Michelangelo Proprietary Uber Uber's platform for Machine Learning, focused on sharing feature pipelines with various teams. (Not open for public usage) Zipline Proprietary AirBnB Airbnb\u2019s Declarative Feature Engineering Framework (Not open for public usage) Metaflow Proprietary Netflix Netflix's human friendly Python/R library for Machine Learning. Has robust Feature Engineering and other attributes. (Open for public usage and contribution) Feast Open-source Feast-dev, Tecton Popular open-source Feature Store. Very complete and competent data platform with Python, Spark and Redis. Integrates with many systems and is very customizable. Can be set up with Kubernetes. Hopsworks Open-source LogicalClocks Open-source Feature Store. Used by Amazon Sagemaker. Very hardware demanding. Butterfree Open-source QuintoAndar Open-source tool used for building Feature Stores using Python and Spark.","title":"Feature Storing"},{"location":"old/MLOps/FeatureStore/#feature-storing","text":"","title":"Feature Storing"},{"location":"old/MLOps/FeatureStore/#what-is-a-feature-store","text":"Feature Stores are components of data architecture that are becoming increasingly popular in the Machine Learning and MLOps environment. The goal of a Feature Store is to process data from various data sources at the same time and turn it into features, which will be consumed by the model training pipeline and the model serving. The concept of Feature Stores is novice and rapidly changing, therefore this page has the objective of showing the key features that are more common among the main Feature Stores in the market, but at the same time it is important to note that some of the tools and frameworks in the market might not comprehend all those exact characteristics in the same manner.","title":"What is a Feature Store?"},{"location":"old/MLOps/FeatureStore/#why-it-matters","text":"Feature Stores can be very useful for Machine Learning in production and are very reliable ways to manage features for research and training using Offline Stores, as it is to manage the feeding of features to a model served in production using an Online Store. This data component can manage to comprehend a wide spectrum of different projects and necessities, some of which are seen below.","title":"Why it matters?"},{"location":"old/MLOps/FeatureStore/#key-features","text":"Enables features to be shared by multiple teams of Data Scientists working at the same time. Creates a reliable automated preprocess pipeline of large quantities of data. Can use and combine different data sources, such as data lakes, data warehouses and streaming of new data, all at once. Provides relevant and online features to a model in production. Can use a time windows system for Data Scientists to gather features from any point in time. Highly customizable for different model needs of consumption, such as batch or real-time predictions.","title":"Key Features"},{"location":"old/MLOps/FeatureStore/#offline-store-vs-online-store","text":"Feature Stores combine multiple data sources and preprocess those into features, the main types of data are: Batch Data : Usually coming from Data Lakes or Data Warehouses. Those are big chunks of data that have been stored in order to be used by models and are not necessarily updated in real-time. Example: Data from customers of a bank, such as age, country, etc. Real-time Data : Usually coming from Streaming and Log events. Those the online data that are constantly coming from sources like the events logged on a system. Example: A transaction in a bank is logged in real-time and fed to the Feature Store. Those types of data are combined inside and form two types of stores: Offline Stores: Store composed of preprocessed features of Batch Data, used for building a historical source of features, that can be used by Data Scientists in the Model Training pipeline. With it's historical components, in most Feature Stores it can be used to provide a series of features at a given time frame or time point. It is normally stored in data warehouses, like IBM Cloud Object Storage, Apache Hive or S3, or in databases, like PostgreSQL, Cassandra and MySQL, but it can also be used in other kinds of systems, like HDFS. Online Stores: Store composed of data from the Offline Store combined with real-time preprocessed features from streaming data sources. It is built with the objective of being the most up-to-date collection of organized features, which can be used to feed the Model in Production with new features for prediction. It is normally stored in databases for rapid access, like MySQL, Cassandra, Redis, but it can be stored in more complex systems.","title":"Offline Store vs Online Store"},{"location":"old/MLOps/FeatureStore/#common-architecture","text":"","title":"Common Architecture"},{"location":"old/MLOps/FeatureStore/#popular-feature-stores","text":"Many companies relay on proprietary software to develop Feature Stores, since many try to make a component that fits into their exactly use-case, such a as: Tools License Developer Observations Michelangelo Proprietary Uber Uber's platform for Machine Learning, focused on sharing feature pipelines with various teams. (Not open for public usage) Zipline Proprietary AirBnB Airbnb\u2019s Declarative Feature Engineering Framework (Not open for public usage) Metaflow Proprietary Netflix Netflix's human friendly Python/R library for Machine Learning. Has robust Feature Engineering and other attributes. (Open for public usage and contribution) Feast Open-source Feast-dev, Tecton Popular open-source Feature Store. Very complete and competent data platform with Python, Spark and Redis. Integrates with many systems and is very customizable. Can be set up with Kubernetes. Hopsworks Open-source LogicalClocks Open-source Feature Store. Used by Amazon Sagemaker. Very hardware demanding. Butterfree Open-source QuintoAndar Open-source tool used for building Feature Stores using Python and Spark.","title":"Popular Feature Stores"},{"location":"old/MLOps/Monitoring/","text":"Continuous Monitoring Machine Learning models are unique software entities as compared to traditional code and their performance can fluctuate over time due to changes in the data input into the model after deployment. So, once a model has been deployed, it needs to be monitored to assure that it performs as expected. It is also necessary to emphasize the importance of monitoring models in production to avoid discriminatory behavior on the part of predictive models. This type of behavior occurs in such a way that an arbitrary group of people is privileged at the expense of others and is usually an unintended result of how the data is collected, selected and used to train the models. Therefore, we need tools that can test and monitor models to ensure their best performance, in addition to mitigating regulatory, reputation and operational risks. What to Monitor? The main concepts that should be monitored are the following: Performance : Being able to evaluate a model\u2019s performance based on a group of metrics and logging its decision or outcome can help give directional insights or compared with historical data. These can be used to compare how well different models perform and therefore which one is the best. Data Issues and Threats : Modern models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data that undergo various transformations. With so many moving parts, it\u2019s not unusual for data inconsistencies and errors to reduce model performance, over time, unnoticed. Models are also susceptible to attacks by many means such as injection of data. Explainability : The black-box nature of the models makes them especially difficult to understand and debug, especially in a production environment. Therefore, being able to explain a model\u2019s decision is vital not only for its improvement but also for accountability reasons, especially in financial institutions. Bias : Since ML models capture relationships from training data, it\u2019s likely that they propagate or amplify existing data bias or maybe even introduce new bias. Being able to detect and mitigate bias during the development process is difficult but necessary. Drift : The statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes, producing what is known as concept drift . The following drawing 1 shows that the health of a Machine Learning system relies on hidden characteristics that are not easy to monitor therefore using the analogy of an iceberg. Popular Serving and Monitoring Tools Most tools used for serving Machine Learning models have monitoring tools. In many suites like IBM Watson, Microsoft Azure and Amazon Sagemaker there is components entirely dedicated to monitoring, like IBM Watson OpenScale. In the following table we can see some of the most popular monitoring tools for machine learning models. Tools License Developer Observations IBM Watson OpenScale Proprietary IBM Monitors models deployed to IBM Watson Machine Learning. Monitors fairness, explainability and drift. Has tools for managing and correcting problems or inaccuracies in production. OpenShift Open-source Red Hat Kubernetes based system able to deploy various types of applications. It is platform agnostic and can be used for any type of application. Can be useful when a model is heavily integrated into a microservice environment. Seldon Core Open-source SeldonIO Deploys models into microservices with Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more. Platform agnostic and works with many Machine Learning frameworks Tensorflow Extended (TFX) Open-source Tensorflow Deploy Tensorflow Models as API and has monitoring capabilities. Image from KDnuggets blog post \"A Machine Learning Model Monitoring Checklist: 7 Things to Track\". https://www.kdnuggets.com/2021/03/machine-learning-model-monitoring-checklist.html \u21a9","title":"Continuous Monitoring"},{"location":"old/MLOps/Monitoring/#continuous-monitoring","text":"Machine Learning models are unique software entities as compared to traditional code and their performance can fluctuate over time due to changes in the data input into the model after deployment. So, once a model has been deployed, it needs to be monitored to assure that it performs as expected. It is also necessary to emphasize the importance of monitoring models in production to avoid discriminatory behavior on the part of predictive models. This type of behavior occurs in such a way that an arbitrary group of people is privileged at the expense of others and is usually an unintended result of how the data is collected, selected and used to train the models. Therefore, we need tools that can test and monitor models to ensure their best performance, in addition to mitigating regulatory, reputation and operational risks.","title":"Continuous Monitoring"},{"location":"old/MLOps/Monitoring/#what-to-monitor","text":"The main concepts that should be monitored are the following: Performance : Being able to evaluate a model\u2019s performance based on a group of metrics and logging its decision or outcome can help give directional insights or compared with historical data. These can be used to compare how well different models perform and therefore which one is the best. Data Issues and Threats : Modern models are increasingly driven by complex feature pipelines and automated workflows that involve dynamic data that undergo various transformations. With so many moving parts, it\u2019s not unusual for data inconsistencies and errors to reduce model performance, over time, unnoticed. Models are also susceptible to attacks by many means such as injection of data. Explainability : The black-box nature of the models makes them especially difficult to understand and debug, especially in a production environment. Therefore, being able to explain a model\u2019s decision is vital not only for its improvement but also for accountability reasons, especially in financial institutions. Bias : Since ML models capture relationships from training data, it\u2019s likely that they propagate or amplify existing data bias or maybe even introduce new bias. Being able to detect and mitigate bias during the development process is difficult but necessary. Drift : The statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes, producing what is known as concept drift . The following drawing 1 shows that the health of a Machine Learning system relies on hidden characteristics that are not easy to monitor therefore using the analogy of an iceberg.","title":"What to Monitor?"},{"location":"old/MLOps/Monitoring/#popular-serving-and-monitoring-tools","text":"Most tools used for serving Machine Learning models have monitoring tools. In many suites like IBM Watson, Microsoft Azure and Amazon Sagemaker there is components entirely dedicated to monitoring, like IBM Watson OpenScale. In the following table we can see some of the most popular monitoring tools for machine learning models. Tools License Developer Observations IBM Watson OpenScale Proprietary IBM Monitors models deployed to IBM Watson Machine Learning. Monitors fairness, explainability and drift. Has tools for managing and correcting problems or inaccuracies in production. OpenShift Open-source Red Hat Kubernetes based system able to deploy various types of applications. It is platform agnostic and can be used for any type of application. Can be useful when a model is heavily integrated into a microservice environment. Seldon Core Open-source SeldonIO Deploys models into microservices with Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more. Platform agnostic and works with many Machine Learning frameworks Tensorflow Extended (TFX) Open-source Tensorflow Deploy Tensorflow Models as API and has monitoring capabilities. Image from KDnuggets blog post \"A Machine Learning Model Monitoring Checklist: 7 Things to Track\". https://www.kdnuggets.com/2021/03/machine-learning-model-monitoring-checklist.html \u21a9","title":"Popular Serving and Monitoring Tools"},{"location":"old/MLOps/PipelineAutomation/","text":"Automation Why automate Machine Learning? The automation of machine learning pipelines is highly correlated with the maturity of the project. There are several steps between developing a model and deploying it, and a good part of this process relies on experimentation. Executing this workflow with less manual intervention as possible should result in: Fast deployments More reliable process Easier problem discovering What can be automated? In the table below, it's possible to find machine learning stages and what tasks may be automated: ML Stages Tasks Data Engineering Data acquiring, validation, and processing Model Development Model training, evaluation, and testing Continuous Integration Build and testing Continuous Delivery Deployment new implementation of a model as a service Monitoring Setting alerts based on pre-defined metrics Levels of automation Defining the level of automation has a crucial impact on the business behind the project. For example: data scientists spend a good amount of time searching for good features, and this time can cost too much in resources which can impact directly the business return of investment(ROI). MLOps.org describes a three-level of automation in machine learning projects: Manual Process: Full experimentation pipeline executed manually using Rapid Application Development(RAD) tools, like Jupyter Notebooks. Deployments are also executed manually. Machine Learning automation: Automation of the experimentation pipeline which includes data and model validation. CI/CD pipelines: Automatically build, test and deploy of ML models and ML training pipeline components, providing a fast and reliable deployment. Common Tools for Machine Learning Pipeline Automation Currently there are many tools used to create and automate Machine Learning pipelines, experiments and any type of process. Below there is a list of some of their services. Tools License Developer Observations DVC Open-source Iterative DVC can be used to make Data Pipelines, which can be automated and reproduced. Very useful if already using DVC for Data and Model versioning. Easily configured and run. Language and framework agnostic. Tensorflow Extended (TFX) Open-source Tensorflow Used for production Machine Learning pipelines. Heavily integrated with Google and GCP. Only works with Tensorflow. Kubeflow Open-source Google, Kubeflow Kubeflow can build automated pipelines and experiments. Intended to build a complete end-to-end solution for Machine learning, being able to also serve and monitor models. Uses Kubernetes and is based on Tensorflow Extended. Works with Tensoorflow and Pytorch. MLflow Open-source MLflow Project Open-source platform for the machine learning lifecycle. Can be used with Python, Conda and Docker. Large community.","title":"Automation"},{"location":"old/MLOps/PipelineAutomation/#automation","text":"","title":"Automation"},{"location":"old/MLOps/PipelineAutomation/#why-automate-machine-learning","text":"The automation of machine learning pipelines is highly correlated with the maturity of the project. There are several steps between developing a model and deploying it, and a good part of this process relies on experimentation. Executing this workflow with less manual intervention as possible should result in: Fast deployments More reliable process Easier problem discovering","title":"Why automate Machine Learning?"},{"location":"old/MLOps/PipelineAutomation/#what-can-be-automated","text":"In the table below, it's possible to find machine learning stages and what tasks may be automated: ML Stages Tasks Data Engineering Data acquiring, validation, and processing Model Development Model training, evaluation, and testing Continuous Integration Build and testing Continuous Delivery Deployment new implementation of a model as a service Monitoring Setting alerts based on pre-defined metrics","title":"What can be automated?"},{"location":"old/MLOps/PipelineAutomation/#levels-of-automation","text":"Defining the level of automation has a crucial impact on the business behind the project. For example: data scientists spend a good amount of time searching for good features, and this time can cost too much in resources which can impact directly the business return of investment(ROI). MLOps.org describes a three-level of automation in machine learning projects: Manual Process: Full experimentation pipeline executed manually using Rapid Application Development(RAD) tools, like Jupyter Notebooks. Deployments are also executed manually. Machine Learning automation: Automation of the experimentation pipeline which includes data and model validation. CI/CD pipelines: Automatically build, test and deploy of ML models and ML training pipeline components, providing a fast and reliable deployment.","title":"Levels of automation"},{"location":"old/MLOps/PipelineAutomation/#common-tools-for-machine-learning-pipeline-automation","text":"Currently there are many tools used to create and automate Machine Learning pipelines, experiments and any type of process. Below there is a list of some of their services. Tools License Developer Observations DVC Open-source Iterative DVC can be used to make Data Pipelines, which can be automated and reproduced. Very useful if already using DVC for Data and Model versioning. Easily configured and run. Language and framework agnostic. Tensorflow Extended (TFX) Open-source Tensorflow Used for production Machine Learning pipelines. Heavily integrated with Google and GCP. Only works with Tensorflow. Kubeflow Open-source Google, Kubeflow Kubeflow can build automated pipelines and experiments. Intended to build a complete end-to-end solution for Machine learning, being able to also serve and monitor models. Uses Kubernetes and is based on Tensorflow Extended. Works with Tensoorflow and Pytorch. MLflow Open-source MLflow Project Open-source platform for the machine learning lifecycle. Can be used with Python, Conda and Docker. Large community.","title":"Common Tools for Machine Learning Pipeline Automation"},{"location":"old/Openscale/","text":"Monitoring with IBM OpenScale Setting Up the Environment Creating OpenScale service from the Services Catalog Creating Machine Learning Provider On the Machine Learning Provider Tab, click on the Add machine learning provider button. 2.1 Add a name and description. 2.2 Add connections and Select Deployment Space Under Service Provider , select Watson Machine Learning (V2) from the dropdown. Next select the deployment space your model is located in. Adding to Dashboard 3.1 On the Insights Dashboard , click on the Add to dashboard button. 3.2 Next select the provider you just created, then select your model deployment and click on Configure and then Configure Monitors . 3.3 Select the data and algorithm types, in our example it is a Binary Classification. 3.4 The next step is selecting the training data that can be stored on a Db2 database or in IBM's Cloud Object Storage. 3.5 Now select the Label column (column you want to predict). 3.6 Next we select all the features we want to include as well as indicate which ones are categorical. 3.7 Here we can select Automatic Logging. 3.8 Finally, we can select prediction and probability for the model output. Configuring Monitors We can create monitors for Fairness , Quality . Drift and Explainability . 4.1 Fairness: The monitor checks your deployments for biases. It tracks when the model shows a tendency to provide a favorable (preferable) outcome more often for one group over another. We have to specify which values represent favorable outcomes and then select the features to monitor for bias, in our case we chose to monitor extreme temperatures in the MinTemp and MaxTemp columns. 4.2 Quality: This monitor evaluates how well the model predicts accurate outcomes that match labeled data. It identifies when model quality declines, so we can retrain your model if needed. We can set the Quality Threshold value, which Area under ROC, at 0.8. 4.3 Drift: The drift evaluation measures drop in accuracy by estimating the drop in accuracy from a base accuracy score determined by the training data and also drops in data consistency, by estimating the drop in data consistency by comparing recent model transactions to the training data. We can set the Drift threshold as 20%. 4.4 Explainability: This allows us to reveal which features contributed to the model\u2019s predicted outcome for a transaction and suggests what changes would result in a different outcome. We can set all features as controllable. Logging In the Transactions page, we can see informations about transactions, including a Timestamp, Prediction and Confidence. We can also access and generate Logs via the Python API First we need to initialize the Watson Machine Learning and OpenScale clients as well as the IAMAuthenticator. service_credentials = { \"apikey\": credentials[\"apikey\"], \"url\": \"https://api.aiopenscale.cloud.ibm.com\", } DEPLOYMENT_UID = metadata[\"deployment_uid\"] MODEL_UID = metadata[\"model_uid\"] MODEL_NAME = metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"] SPACE_ID = credentials[\"space_id\"] WOS_GUID = get_instance_guid(api_key=service_credentials[\"apikey\"]) WOS_CREDENTIALS = { \"instance_guid\": WOS_GUID, \"apikey\": service_credentials[\"apikey\"], \"url\": \"https://api.aiopenscale.cloud.ibm.com\", } if WOS_GUID is None: print(\"Watson OpenScale GUID NOT FOUND\") else: print(WOS_GUID) wml_credentials = {\"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"]} wml_client = ibm_watson_machine_learning.APIClient(wml_credentials) wml_credentials = { \"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"], \"instance_id\": \"wml_local\", } wml_client.set.default_space(SPACE_ID) authenticator = IAMAuthenticator(apikey=credentials[\"apikey\"]) wos_client = ibm_watson_openscale.APIClient( authenticator=authenticator, service_url=\"https://api.aiopenscale.cloud.ibm.com\") Then we can get the model's scoring endpoint. for deployment in wml_client.deployments.get_details()['resources']: if DEPLOYMENT_UID in deployment['metadata']['id']: scoring_endpoint = deployment['entity']['status']['online_url']['url'] print(scoring_endpoint) https://us-south.ml.cloud.ibm.com/ml/v4/deployments/e02e481d-4e56-470f-baa9-ae84a583c0a8/predictions Here we display the OpenScale subscriptions. wos_client.subscriptions.show() Now we can load a dataset and then create the request body to make the predictions. df_data = pd.read_csv(\"../data/weatherAUS_processed.csv\") X = df_data.iloc[:, :-1] y = df_data[df_data.columns[-1]] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.01, random_state=1337 ) payload_scoring = { \"input_data\": [ { \"fields\": X.columns.to_numpy().tolist(), \"values\": X_test.to_numpy().tolist(), } ] } Then we send the request to our model. scoring_response = wml_client.deployments.score(DEPLOYMENT_UID, payload_scoring) After that, we use the subscription_id we got from step 3. we get the Payload data set ID. subscription_id = 'bb7a45c3-15ad-4932-aeb8-8d32d54b8b05' payload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, target_target_id=subscription_id, target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id print(\"Payload data set id:\", payload_data_set_id) Payload data set id: f4791725-24f8-4a00-9c13-b331ebca47f6 Now we can manually create logs with the predictions from our model and the data we sent in the request. records = [PayloadRecord(request=payload_scoring, response=scoring_response, response_time=72)] store_record_info = wos_client.data_sets.store_records(payload_data_set_id, records) We also can do the same thing for Feedback datasets, which don't require the model prediction. feedback_dataset = wos_client.data_sets.list(type=DataSetTypes.FEEDBACK, target_target_id=subscription_id, target_target_type=TargetTypes.SUBSCRIPTION).result feedback_dataset_id = feedback_dataset.data_sets[0].metadata.id if feedback_dataset_id is None: print(\"Feedback data set not found. Please check quality monitor status.\") sys.exit(1) data = X_test.to_dict('records') wos_client.data_sets.store_records( feedback_dataset_id, request_body=data, background_mode=False, header=True, delimiter=',', csv_max_line_length=1000) print(wos_client.data_sets.get_records_count(data_set_id=feedback_dataset_id)) After that we can access these datasets as Pandas dataframes. records2 = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id,output_type=ResponseTypes.PANDAS) df = records2.result Then we can use that Pandas dataframe to create plots or other forms of analysis. import matplotlib.pyplot as plt plt.hist(df.prediction_probability) plt.legend(title='Predictions Probability Histogram') plt.show() Evaluating Model On the main Insights Dashboard when click on our deployment, we can evaluate or model by clicking on the Actions button on the top-right and then Evaluate now in dropdown, where we can import a test dataset by either directly uploading a .csv file or by using dataset or database stored in the IBM COS. After that, the metrics we defined for the monitors will be used to generate reports depicting our model's performance. Explaining Predictions Again, in the Transactions page, we can click on the Explain button, in the following page we can observe each features' relative weight indicating how strongly they influenced the model\u2019s predicted outcome. In the Inspect tab, there is a table displaying the values each feature would have to have to alter the prediction result, here we can also change the values by hand to see what the outcome would be.","title":"Monitoring with IBM OpenScale"},{"location":"old/Openscale/#monitoring-with-ibm-openscale","text":"","title":"Monitoring with IBM OpenScale"},{"location":"old/Openscale/#setting-up-the-environment","text":"Creating OpenScale service from the Services Catalog Creating Machine Learning Provider On the Machine Learning Provider Tab, click on the Add machine learning provider button. 2.1 Add a name and description. 2.2 Add connections and Select Deployment Space Under Service Provider , select Watson Machine Learning (V2) from the dropdown. Next select the deployment space your model is located in. Adding to Dashboard 3.1 On the Insights Dashboard , click on the Add to dashboard button. 3.2 Next select the provider you just created, then select your model deployment and click on Configure and then Configure Monitors . 3.3 Select the data and algorithm types, in our example it is a Binary Classification. 3.4 The next step is selecting the training data that can be stored on a Db2 database or in IBM's Cloud Object Storage. 3.5 Now select the Label column (column you want to predict). 3.6 Next we select all the features we want to include as well as indicate which ones are categorical. 3.7 Here we can select Automatic Logging. 3.8 Finally, we can select prediction and probability for the model output. Configuring Monitors We can create monitors for Fairness , Quality . Drift and Explainability . 4.1 Fairness: The monitor checks your deployments for biases. It tracks when the model shows a tendency to provide a favorable (preferable) outcome more often for one group over another. We have to specify which values represent favorable outcomes and then select the features to monitor for bias, in our case we chose to monitor extreme temperatures in the MinTemp and MaxTemp columns. 4.2 Quality: This monitor evaluates how well the model predicts accurate outcomes that match labeled data. It identifies when model quality declines, so we can retrain your model if needed. We can set the Quality Threshold value, which Area under ROC, at 0.8. 4.3 Drift: The drift evaluation measures drop in accuracy by estimating the drop in accuracy from a base accuracy score determined by the training data and also drops in data consistency, by estimating the drop in data consistency by comparing recent model transactions to the training data. We can set the Drift threshold as 20%. 4.4 Explainability: This allows us to reveal which features contributed to the model\u2019s predicted outcome for a transaction and suggests what changes would result in a different outcome. We can set all features as controllable.","title":"Setting Up the Environment"},{"location":"old/Openscale/#logging","text":"In the Transactions page, we can see informations about transactions, including a Timestamp, Prediction and Confidence.","title":"Logging"},{"location":"old/Openscale/#we-can-also-access-and-generate-logs-via-the-python-api","text":"First we need to initialize the Watson Machine Learning and OpenScale clients as well as the IAMAuthenticator. service_credentials = { \"apikey\": credentials[\"apikey\"], \"url\": \"https://api.aiopenscale.cloud.ibm.com\", } DEPLOYMENT_UID = metadata[\"deployment_uid\"] MODEL_UID = metadata[\"model_uid\"] MODEL_NAME = metadata[\"project_name\"] + \"_\" + metadata[\"project_version\"] SPACE_ID = credentials[\"space_id\"] WOS_GUID = get_instance_guid(api_key=service_credentials[\"apikey\"]) WOS_CREDENTIALS = { \"instance_guid\": WOS_GUID, \"apikey\": service_credentials[\"apikey\"], \"url\": \"https://api.aiopenscale.cloud.ibm.com\", } if WOS_GUID is None: print(\"Watson OpenScale GUID NOT FOUND\") else: print(WOS_GUID) wml_credentials = {\"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"]} wml_client = ibm_watson_machine_learning.APIClient(wml_credentials) wml_credentials = { \"url\": credentials[\"url\"], \"apikey\": credentials[\"apikey\"], \"instance_id\": \"wml_local\", } wml_client.set.default_space(SPACE_ID) authenticator = IAMAuthenticator(apikey=credentials[\"apikey\"]) wos_client = ibm_watson_openscale.APIClient( authenticator=authenticator, service_url=\"https://api.aiopenscale.cloud.ibm.com\") Then we can get the model's scoring endpoint. for deployment in wml_client.deployments.get_details()['resources']: if DEPLOYMENT_UID in deployment['metadata']['id']: scoring_endpoint = deployment['entity']['status']['online_url']['url'] print(scoring_endpoint) https://us-south.ml.cloud.ibm.com/ml/v4/deployments/e02e481d-4e56-470f-baa9-ae84a583c0a8/predictions Here we display the OpenScale subscriptions. wos_client.subscriptions.show() Now we can load a dataset and then create the request body to make the predictions. df_data = pd.read_csv(\"../data/weatherAUS_processed.csv\") X = df_data.iloc[:, :-1] y = df_data[df_data.columns[-1]] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.01, random_state=1337 ) payload_scoring = { \"input_data\": [ { \"fields\": X.columns.to_numpy().tolist(), \"values\": X_test.to_numpy().tolist(), } ] } Then we send the request to our model. scoring_response = wml_client.deployments.score(DEPLOYMENT_UID, payload_scoring) After that, we use the subscription_id we got from step 3. we get the Payload data set ID. subscription_id = 'bb7a45c3-15ad-4932-aeb8-8d32d54b8b05' payload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, target_target_id=subscription_id, target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id print(\"Payload data set id:\", payload_data_set_id) Payload data set id: f4791725-24f8-4a00-9c13-b331ebca47f6 Now we can manually create logs with the predictions from our model and the data we sent in the request. records = [PayloadRecord(request=payload_scoring, response=scoring_response, response_time=72)] store_record_info = wos_client.data_sets.store_records(payload_data_set_id, records) We also can do the same thing for Feedback datasets, which don't require the model prediction. feedback_dataset = wos_client.data_sets.list(type=DataSetTypes.FEEDBACK, target_target_id=subscription_id, target_target_type=TargetTypes.SUBSCRIPTION).result feedback_dataset_id = feedback_dataset.data_sets[0].metadata.id if feedback_dataset_id is None: print(\"Feedback data set not found. Please check quality monitor status.\") sys.exit(1) data = X_test.to_dict('records') wos_client.data_sets.store_records( feedback_dataset_id, request_body=data, background_mode=False, header=True, delimiter=',', csv_max_line_length=1000) print(wos_client.data_sets.get_records_count(data_set_id=feedback_dataset_id)) After that we can access these datasets as Pandas dataframes. records2 = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id,output_type=ResponseTypes.PANDAS) df = records2.result Then we can use that Pandas dataframe to create plots or other forms of analysis. import matplotlib.pyplot as plt plt.hist(df.prediction_probability) plt.legend(title='Predictions Probability Histogram') plt.show()","title":"We can also access and generate Logs via the Python API"},{"location":"old/Openscale/#evaluating-model","text":"On the main Insights Dashboard when click on our deployment, we can evaluate or model by clicking on the Actions button on the top-right and then Evaluate now in dropdown, where we can import a test dataset by either directly uploading a .csv file or by using dataset or database stored in the IBM COS. After that, the metrics we defined for the monitors will be used to generate reports depicting our model's performance.","title":"Evaluating Model"},{"location":"old/Openscale/#explaining-predictions","text":"Again, in the Transactions page, we can click on the Explain button, in the following page we can observe each features' relative weight indicating how strongly they influenced the model\u2019s predicted outcome. In the Inspect tab, there is a table displaying the values each feature would have to have to alter the prediction result, here we can also change the values by hand to see what the outcome would be.","title":"Explaining Predictions"},{"location":"old/Structure/project_structure/","text":"Tools and Project Structure In the following sections we will go over the steps for the implementation of a MLOps Proof-of-Concept pipeline using IBM Watson tools and services. A template repository with a complete MLOps cycle: versioning data, generating reports on pull requests and deploying the model on releases with DVC and CML using Github Actions and IBM Watson as well as instructions to run the project can be found here . Note We won't get into how to create predictive models or preprocessing data, since our main objective is to discuss MLOps and create a development cycle using those concepts. Project Tools The main tools discussed in the guide are shown in the following table. As the guide is intended to be modular, a team can swap tools for others depending on the project necessities or preferences. Tools Function Developer IBM Watson ML Deploying model as API IBM IBM Watson OpenScale Monitoring Model in production IBM DVC Data and Model Versioning Iterative CML Pipeline Automation Iterative Terraform Setups IBM infrastructure with script HashiCorp Github Code versioning Github Github Actions CI/CD Automation Github Pytest Python script testing Pytest-dev Pre-commit Running tests on local commit Pre-commit Cookiecutter Creating folder structure and files Cookiecutter Folder Structure The above image is the project's folder structure, we'll talk about each specific part in further details trough out the guide. data , models and results contain files which are being stored and versioned by DVC. notebooks contain Jupyter Notebooks used for the exploratory analysis, development of models, or data manipulation. src contains scripts for training and evaluating the model as well as tests and scripts for pipelines and APIs. This folder structure is going to be implemented in a blank project in Introduction/Starting a New Project with Cookiecutter Requirements The requirements file is a list of all of a project\u2019s dependencies and the specific version of each dependency, including the dependencies needed by the dependencies. It can also be used to create a virtual environment. This is extremely important to avoid conflicts between Python libraries and also ensure the experiments can be reproduced in different machines. Metadata File To keep track of the model information we have a metadata.yaml file, this helps with CI/CD and pipeline automation. Such as updating or deploying the model without the need of user input. author : guipleite datetime_creted : 29/03/2021_13:46:23:802394723 model_type : scikit-learn_0.23 project_name : Rain_aus project_version : v0.3 deployment_uid : e02e481d-4a56-470f-baa9-ae84a583c0a8 model_uid : f29e4cfc-3aab-438a-b703-fabc265f43a3 Using Jupyter Notebooks vs. Python Scripts The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It is widely used in the fields of Data Science and Machine learning for its versatility in development and documentation of projects, however the usage of notebooks may cause some problems for our development cycle: Versioning : Since notebooks source code are much more complex , we can't easily visualize the difference between versions using git. There are some tools that can help with that, however. Reproducibility : A great feature of notebooks is being able to run cells in a non-sequential order, but this is a big problem if we want to reproduce the code, since it's hard to know in what order or which cells where executed, this is especially bad if we want to automate pipeline. Standardized In/Out : By using scripts we can create pipelines with standardized entries and exits, therefore, we can create universal pipelines since no matter the model what it will receive and return will be in the same format. Access to Functions : In the model.py script, we define the train and evaluate function, where the model is declared and trained and the metrics for the evaluation are defined. These functions can be called by other scripts such as train.py and evaluate.py so we can create pipelines to train the model on a remote instance or evaluate an already trained model file in a consistent form. def train(data, params): ... return pipeline, logs def evaluate(data, pipeline, OUTPUT_PATH): ... return results In our project we choose to use scripts instead of Jupyter Notebooks for the reasons cited above, however notebooks could still be used as a form of experimentation of models or processes and the script as a more 'definitive' form.","title":"Tools and Project Structure"},{"location":"old/Structure/project_structure/#tools-and-project-structure","text":"In the following sections we will go over the steps for the implementation of a MLOps Proof-of-Concept pipeline using IBM Watson tools and services. A template repository with a complete MLOps cycle: versioning data, generating reports on pull requests and deploying the model on releases with DVC and CML using Github Actions and IBM Watson as well as instructions to run the project can be found here . Note We won't get into how to create predictive models or preprocessing data, since our main objective is to discuss MLOps and create a development cycle using those concepts.","title":"Tools and Project Structure"},{"location":"old/Structure/project_structure/#project-tools","text":"The main tools discussed in the guide are shown in the following table. As the guide is intended to be modular, a team can swap tools for others depending on the project necessities or preferences. Tools Function Developer IBM Watson ML Deploying model as API IBM IBM Watson OpenScale Monitoring Model in production IBM DVC Data and Model Versioning Iterative CML Pipeline Automation Iterative Terraform Setups IBM infrastructure with script HashiCorp Github Code versioning Github Github Actions CI/CD Automation Github Pytest Python script testing Pytest-dev Pre-commit Running tests on local commit Pre-commit Cookiecutter Creating folder structure and files Cookiecutter","title":"Project Tools"},{"location":"old/Structure/project_structure/#folder-structure","text":"The above image is the project's folder structure, we'll talk about each specific part in further details trough out the guide. data , models and results contain files which are being stored and versioned by DVC. notebooks contain Jupyter Notebooks used for the exploratory analysis, development of models, or data manipulation. src contains scripts for training and evaluating the model as well as tests and scripts for pipelines and APIs. This folder structure is going to be implemented in a blank project in Introduction/Starting a New Project with Cookiecutter","title":"Folder Structure"},{"location":"old/Structure/project_structure/#requirements","text":"The requirements file is a list of all of a project\u2019s dependencies and the specific version of each dependency, including the dependencies needed by the dependencies. It can also be used to create a virtual environment. This is extremely important to avoid conflicts between Python libraries and also ensure the experiments can be reproduced in different machines.","title":"Requirements"},{"location":"old/Structure/project_structure/#metadata-file","text":"To keep track of the model information we have a metadata.yaml file, this helps with CI/CD and pipeline automation. Such as updating or deploying the model without the need of user input. author : guipleite datetime_creted : 29/03/2021_13:46:23:802394723 model_type : scikit-learn_0.23 project_name : Rain_aus project_version : v0.3 deployment_uid : e02e481d-4a56-470f-baa9-ae84a583c0a8 model_uid : f29e4cfc-3aab-438a-b703-fabc265f43a3","title":"Metadata File"},{"location":"old/Structure/project_structure/#using-jupyter-notebooks-vs-python-scripts","text":"The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It is widely used in the fields of Data Science and Machine learning for its versatility in development and documentation of projects, however the usage of notebooks may cause some problems for our development cycle: Versioning : Since notebooks source code are much more complex , we can't easily visualize the difference between versions using git. There are some tools that can help with that, however. Reproducibility : A great feature of notebooks is being able to run cells in a non-sequential order, but this is a big problem if we want to reproduce the code, since it's hard to know in what order or which cells where executed, this is especially bad if we want to automate pipeline. Standardized In/Out : By using scripts we can create pipelines with standardized entries and exits, therefore, we can create universal pipelines since no matter the model what it will receive and return will be in the same format. Access to Functions : In the model.py script, we define the train and evaluate function, where the model is declared and trained and the metrics for the evaluation are defined. These functions can be called by other scripts such as train.py and evaluate.py so we can create pipelines to train the model on a remote instance or evaluate an already trained model file in a consistent form. def train(data, params): ... return pipeline, logs def evaluate(data, pipeline, OUTPUT_PATH): ... return results In our project we choose to use scripts instead of Jupyter Notebooks for the reasons cited above, however notebooks could still be used as a form of experimentation of models or processes and the script as a more 'definitive' form.","title":"Using Jupyter Notebooks vs. Python Scripts"},{"location":"old/Structure/starting/","text":"Starting a New Project with Cookiecutter What is Cookiecutter? Cookiecutter is a CLI tool that can be used to create projects based on templates. It can create folder structures and static files based on user input info on predefined questions. In this guide cookiecutter will create the project structure based on the MLOps Cookiecutter Template Installing Cookiecutter Cookiecutter is officially supported on Linux, MacOS and Windows. More info on installing it can be accessed on their documentation Python pip install --user cookiecutter or pip3 install --user cookiecutter Alternative: Homebrew (MacOS only) brew install cookiecutter Alternative: Debian/Ubuntu sudo apt-get install cookiecutter Creating a New Project Now that cookiecutter is configured we can use the template to create a structured new project cookiecutter https://github.com/mlops-guide/mlops-template.git This should result in the following questions, which will be used to fill the project with info author [ mlops-guide ] : project_name [ Australia Weather Prediction ] : project_slug [ australia_weather_prediction ] : project_version [ v0.1 ] : model_type [ scikit-learn_0.23 ] : Select open_source_license: 1 - MIT 2 - BSD-3-Clause 3 - No license file Choose from 1 , 2 , 3 [ 1 ] : use_github_actions_for_CICD [ y ] : use_pytest [ y ] : use_black [ y ] : Basic Structure After running cookiecutter, the project tree should be as the following. You can check this by running $ tree on Linux, using Finder on MacOS or File System on Windows. . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 README.md \u251c\u2500\u2500 metadata.yaml \u251c\u2500\u2500 models \u2502 \u2514\u2500\u2500 README.md \u251c\u2500\u2500 notebooks \u2502 \u2514\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 results \u2502 \u2514\u2500\u2500 README.md \u2514\u2500\u2500 src \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 README.md \u2514\u2500\u2500 tests \u251c\u2500\u2500 README.md \u2514\u2500\u2500 test_australia_weather_prediction.py 7 directories, 11 files","title":"Starting a New Project with Cookiecutter"},{"location":"old/Structure/starting/#starting-a-new-project-with-cookiecutter","text":"","title":"Starting a New Project with Cookiecutter"},{"location":"old/Structure/starting/#what-is-cookiecutter","text":"Cookiecutter is a CLI tool that can be used to create projects based on templates. It can create folder structures and static files based on user input info on predefined questions. In this guide cookiecutter will create the project structure based on the MLOps Cookiecutter Template","title":"What is Cookiecutter?"},{"location":"old/Structure/starting/#installing-cookiecutter","text":"Cookiecutter is officially supported on Linux, MacOS and Windows. More info on installing it can be accessed on their documentation","title":"Installing Cookiecutter"},{"location":"old/Structure/starting/#python","text":"pip install --user cookiecutter or pip3 install --user cookiecutter","title":"Python"},{"location":"old/Structure/starting/#alternative-homebrew-macos-only","text":"brew install cookiecutter","title":"Alternative: Homebrew (MacOS only)"},{"location":"old/Structure/starting/#alternative-debianubuntu","text":"sudo apt-get install cookiecutter","title":"Alternative: Debian/Ubuntu"},{"location":"old/Structure/starting/#creating-a-new-project","text":"Now that cookiecutter is configured we can use the template to create a structured new project cookiecutter https://github.com/mlops-guide/mlops-template.git This should result in the following questions, which will be used to fill the project with info author [ mlops-guide ] : project_name [ Australia Weather Prediction ] : project_slug [ australia_weather_prediction ] : project_version [ v0.1 ] : model_type [ scikit-learn_0.23 ] : Select open_source_license: 1 - MIT 2 - BSD-3-Clause 3 - No license file Choose from 1 , 2 , 3 [ 1 ] : use_github_actions_for_CICD [ y ] : use_pytest [ y ] : use_black [ y ] :","title":"Creating a New Project"},{"location":"old/Structure/starting/#basic-structure","text":"After running cookiecutter, the project tree should be as the following. You can check this by running $ tree on Linux, using Finder on MacOS or File System on Windows. . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 README.md \u251c\u2500\u2500 metadata.yaml \u251c\u2500\u2500 models \u2502 \u2514\u2500\u2500 README.md \u251c\u2500\u2500 notebooks \u2502 \u2514\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 results \u2502 \u2514\u2500\u2500 README.md \u2514\u2500\u2500 src \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 README.md \u2514\u2500\u2500 tests \u251c\u2500\u2500 README.md \u2514\u2500\u2500 test_australia_weather_prediction.py 7 directories, 11 files","title":"Basic Structure"},{"location":"old/Testes/","text":"Testes de Markdown Essa p\u00e1gina \u00e9 dedicada a testes de Markdown e ferramentas complementares que possam ser interessantes ao guia. A partir dos testes \u00e9 poss\u00edvel exibir as ferramentas para os integrantes do grupo decidirem sobre sua utiliza\u00e7\u00e3o ASCII Cinema Melhorado Como usar Grave o v\u00eddeo com o ASCII cinema record https://asciinema.org/ Salve o o arquivo .cast na pasta ASCII_cinema Acesse dessa maneira: <asciinema-player rows=18 theme=\"monokai\" cols=200 src=\"/ASCII_cinema/example.cast\"></asciinema-player>","title":"Testes de Markdown"},{"location":"old/Testes/#testes-de-markdown","text":"Essa p\u00e1gina \u00e9 dedicada a testes de Markdown e ferramentas complementares que possam ser interessantes ao guia. A partir dos testes \u00e9 poss\u00edvel exibir as ferramentas para os integrantes do grupo decidirem sobre sua utiliza\u00e7\u00e3o","title":"Testes de Markdown"},{"location":"old/Testes/#ascii-cinema-melhorado","text":"","title":"ASCII Cinema Melhorado"},{"location":"old/Testes/#como-usar","text":"Grave o v\u00eddeo com o ASCII cinema record https://asciinema.org/ Salve o o arquivo .cast na pasta ASCII_cinema Acesse dessa maneira: <asciinema-player rows=18 theme=\"monokai\" cols=200 src=\"/ASCII_cinema/example.cast\"></asciinema-player>","title":"Como usar"},{"location":"old/Versionamento/","text":"What is DVC? In the MLOps Concepts section we were able to show how versioning data, models, and pipelines are so important in an ML project. In this section, we will cover DVC and how you can set up this tool to version your data, models and automatize pipelines. DVC, which goes by Data Version Control, is essentially an experiment management tool for ML projects. DVC software is built upon Git and its main goal is to codify data, models and pipelines through the command line. This is all possible because DVC replaces large files (such as datasets and ML models) with small metafiles that point to the original data. By doing that, it's possible to keep these metafiles along with the source code of the project in a repository while large files are kept in at a remote data storage. DVC workflow from https://dvc.org Since DVC works on top of Git, its syntax and workflow are also similar so that you're able to treat data and model versioning just as you do with code. Although DVC can work stand-alone, it's highly recommended to work alongside Git. DVC can also manage the project's pipelines to make experiments reproducible for all members. These pipelines are lightweight and are created using dependency graphs. It's important to note that DVC is free, open-source and platform agnostic, so it runs with a diverse option of OS, programming languages and ML libraries. Install DVC can be installed as a Python Library with pip package manager: $ pip install dvc Depending on which remote storage interface you're using, it's important to install optional dependencies(s3, azure, gdrive, gs, oss, ssh or all). In this project we are using S3 interface to connect with IBM Cloud Object Storage. $ pip install \"dvc[s3]\" It's also possible to install DVC using conda: $ conda install -c conda-forge mamba $ mamba install -c conda-forge dvc $ mamba install -c conda-forge dvc-s3 Project Setup Initialization After installing DVC, you can go to your project's folder and initialize both Git and DVC: $ git init $ dvc init If you run git status , you can check DVC's initial config files that were created. Remote Storage Finally, we are going to setup our remote storage to keep our datasets and models. With dvc remote add you can point to your remote storage. $ dvc remote add -d remote-storage s3://bucket_namme/folder/ DVC uses by default AWS CLI to authenticate. IBM Cloud Object Storage works with S3 interface and also AWS, the only additional step here is to configure the IBM endpoint and configure the credentials. $ dvc remote modify remote-storage endpointurl \\ https://s3.us-south.cloud-object-storage.appdomain.cloud To configure the credentials you can use default ~/.aws/credentials file and just add a new profile, or point to a new credential path: Run: $ dvc remote modify myremote profile myprofile Modify ~/.aws/credentials [default] aws_access_key_id = ************ aws_secret_access_key = ************ [myprofile] aws_access_key_id = ************ aws_secret_access_key = ************ or $ dvc remote modify credentialpath /path/to/creds If you have any problems trying to configure your remote storage, go check DVC docs for remote command Data Versioning Working with Pipelines","title":"What is DVC?"},{"location":"old/Versionamento/#what-is-dvc","text":"In the MLOps Concepts section we were able to show how versioning data, models, and pipelines are so important in an ML project. In this section, we will cover DVC and how you can set up this tool to version your data, models and automatize pipelines. DVC, which goes by Data Version Control, is essentially an experiment management tool for ML projects. DVC software is built upon Git and its main goal is to codify data, models and pipelines through the command line. This is all possible because DVC replaces large files (such as datasets and ML models) with small metafiles that point to the original data. By doing that, it's possible to keep these metafiles along with the source code of the project in a repository while large files are kept in at a remote data storage.","title":"What is DVC?"},{"location":"old/Versionamento/#install","text":"DVC can be installed as a Python Library with pip package manager: $ pip install dvc Depending on which remote storage interface you're using, it's important to install optional dependencies(s3, azure, gdrive, gs, oss, ssh or all). In this project we are using S3 interface to connect with IBM Cloud Object Storage. $ pip install \"dvc[s3]\" It's also possible to install DVC using conda: $ conda install -c conda-forge mamba $ mamba install -c conda-forge dvc $ mamba install -c conda-forge dvc-s3","title":"Install"},{"location":"old/Versionamento/#project-setup","text":"","title":"Project Setup"},{"location":"old/Versionamento/#initialization","text":"After installing DVC, you can go to your project's folder and initialize both Git and DVC: $ git init $ dvc init If you run git status , you can check DVC's initial config files that were created.","title":"Initialization"},{"location":"old/Versionamento/#remote-storage","text":"Finally, we are going to setup our remote storage to keep our datasets and models. With dvc remote add you can point to your remote storage. $ dvc remote add -d remote-storage s3://bucket_namme/folder/ DVC uses by default AWS CLI to authenticate. IBM Cloud Object Storage works with S3 interface and also AWS, the only additional step here is to configure the IBM endpoint and configure the credentials. $ dvc remote modify remote-storage endpointurl \\ https://s3.us-south.cloud-object-storage.appdomain.cloud To configure the credentials you can use default ~/.aws/credentials file and just add a new profile, or point to a new credential path: Run: $ dvc remote modify myremote profile myprofile Modify ~/.aws/credentials [default] aws_access_key_id = ************ aws_secret_access_key = ************ [myprofile] aws_access_key_id = ************ aws_secret_access_key = ************ or $ dvc remote modify credentialpath /path/to/creds If you have any problems trying to configure your remote storage, go check DVC docs for remote command Data Versioning Working with Pipelines","title":"Remote Storage"},{"location":"old/Versionamento/basic_dvc/","text":"Data Versioning So now that your repository is initialized and connected with your remote storage, we can start to version your data. Warning This section will use the help of the template repository to show how to version data with DVC. Feel free to reproduce it with your own data files for your project. dvc add Suppose you have downloaded our weatherAUS.csv or another file inside your data folder and you want to add this file under the data version control of your project. The first step is to put this file under DVC local control and DVC cache by running: $ dvc add data/weatherAUS.csv dvc add works the same way git add command. Your dataset is now under DVC local control and DVC cache(which is by default local but can be configured to be shared). But now, you need to put your data under Git version control with git add . Note that DVC created two files named weatherAUS.csv.dvc and .gitignore in your data folder. These files are responsible for codifying your data: weatherAUS.csv.dvc: This file points where your actual data is and every time that your data change, this file changes too. .gitignore: This file won't allow git to upload your data file to your repository. DVC creates automatically so you won't need to worry about it. These metafiles with .dvc extension are YAML files that contain some key-value pair information about your data or model. Here it's an example: outs : - md5 : a304afb96060aad90176268345e10355 path : weatherAUS.csv The md5 is a very common hash function that takes a file content and produces a string of thirty-two characters. So if you make just a small change in a data file or model controlled by DVC, the md5 hash will be recalculated and that's how your colleagues will keep track of what's new in your experiment. If you are interested in all .dvc arguments, check out the official docs . dvc checkout To see how DVC has your data under control, you can remove your weatherAUS.csv . After that, try running dvc checkout data/weatherAUS.csv.dvc and you will see that your dataset is back! dvc push So now DVC and Git have your data under control. To finally allocated this data in your remote storage you need to run: $ dvc push data/weatherAUS.csv After that, your data is already in your remote storage and you just need to let Git know that by: $ git commit -m 'data push' dvc pull Finally, you can checkout to your experiment branch and pull your dataset related to that experiment: $ git checkout branch_name And to pull the dataset from your remote storage, just run: $ dvc pull","title":"Data Versioning"},{"location":"old/Versionamento/basic_dvc/#data-versioning","text":"So now that your repository is initialized and connected with your remote storage, we can start to version your data. Warning This section will use the help of the template repository to show how to version data with DVC. Feel free to reproduce it with your own data files for your project.","title":"Data Versioning"},{"location":"old/Versionamento/basic_dvc/#dvc-add","text":"Suppose you have downloaded our weatherAUS.csv or another file inside your data folder and you want to add this file under the data version control of your project. The first step is to put this file under DVC local control and DVC cache by running: $ dvc add data/weatherAUS.csv dvc add works the same way git add command. Your dataset is now under DVC local control and DVC cache(which is by default local but can be configured to be shared). But now, you need to put your data under Git version control with git add . Note that DVC created two files named weatherAUS.csv.dvc and .gitignore in your data folder. These files are responsible for codifying your data: weatherAUS.csv.dvc: This file points where your actual data is and every time that your data change, this file changes too. .gitignore: This file won't allow git to upload your data file to your repository. DVC creates automatically so you won't need to worry about it. These metafiles with .dvc extension are YAML files that contain some key-value pair information about your data or model. Here it's an example: outs : - md5 : a304afb96060aad90176268345e10355 path : weatherAUS.csv The md5 is a very common hash function that takes a file content and produces a string of thirty-two characters. So if you make just a small change in a data file or model controlled by DVC, the md5 hash will be recalculated and that's how your colleagues will keep track of what's new in your experiment. If you are interested in all .dvc arguments, check out the official docs .","title":"dvc add"},{"location":"old/Versionamento/basic_dvc/#dvc-checkout","text":"To see how DVC has your data under control, you can remove your weatherAUS.csv . After that, try running dvc checkout data/weatherAUS.csv.dvc and you will see that your dataset is back!","title":"dvc checkout"},{"location":"old/Versionamento/basic_dvc/#dvc-push","text":"So now DVC and Git have your data under control. To finally allocated this data in your remote storage you need to run: $ dvc push data/weatherAUS.csv After that, your data is already in your remote storage and you just need to let Git know that by: $ git commit -m 'data push'","title":"dvc push"},{"location":"old/Versionamento/basic_dvc/#dvc-pull","text":"Finally, you can checkout to your experiment branch and pull your dataset related to that experiment: $ git checkout branch_name And to pull the dataset from your remote storage, just run: $ dvc pull","title":"dvc pull"},{"location":"old/Versionamento/pipelines_dvc/","text":"Working with Pipelines After learning how to version data or models using DVC it's time to build your experiment pipeline. Let's assume that we are using the last section dataset as a data source for training a classification model. Let's also consider that we have three stages in this experiment: Preprocessing your data(extract features...) Train the model Evaluate the model Here you should have in hands our scripts: preprocess.py , train.py , evaluate.py and model.py . Warning Just as in the last section, we will use the help of the template repository to explain and build DVC's pipelines. Feel free to use your scripts and create specific pipelines for your project needs. Creating pipelines DVC builds a pipeline based on three components: Inputs, Outputs, and Command. So for the preprocessing stage, this would look like this: Inputs: weatherAUS.csv.csv and preprocess.py script Outputs: weatherAUS_processed.csv Command: python preprocess.py weatherAUS.csv So to create this stage of preprocessing, we use dvc run : dvc run -n preprocess \\ -d ./src/preprocess_data.py -d data/weatherAUS.csv \\ -o ./data/weatherAUS_processed.csv \\ python3 ./src/preprocess_data.py ./data/weatherAUS.csv We named this stage \"preprocess\" by using the flag -n . We also defined this stage inputs with the flag -d and the outputs with the flag -o . The command will always be the last piece of dvc run without any flag. Tip Output files are added to DVC control when reproducing a DVC stage. When finalizing your experiment remember to use dvc push to version not only the data used but those outputs generated from the experiment. The train stages would also be created using dvc run : dvc run -n train \\ -d ./src/train.py -d ./data/weatherAUS_processed.csv -d ./src/model.py \\ -o ./models/model.joblib \\ python3 ./src/train.py ./data/weatherAUS_processed.csv ./src/model.py 200 Warning The number 200 at dvc run above is related to our script function. If your are using your own script just ignore it. At this point, you might have noticed that two new files were created: dvc.yaml and dvc.lock . The first one will be responsible for saving what was described in each dvc run command. So if you wanna create or change a specific stage, it's possible to just edit dvc.yaml . Our current file would look like this: stages : preprocess : cmd : python3 ./src/preprocess_data.py ./data/weatherAUS.csv deps : - ./src/preprocess_data.py - data/weatherAUS.csv outs : - ./data/weatherAUS_processed.csv - ./data/features.csv train : cmd : python3 ./src/train.py ./data/weatherAUS_processed.csv ./src/model.py 200 deps : - ./data/weatherAUS_processed.csv - ./src/model.py - ./src/train.py outs : - ./models/model.joblib The second file created is dvc.lock . This is also a YAML file and its function is similar to .dvc files. Inside, we can find the path and a hash code for each file of each stage so DVC can track changes. Tracking these changes is important because now DVC will know when a stage needs to be rerun or not. Your currently pipeline looks likes this: Saving metrics Finally, let's create our last stage so we can evaluate our model: dvc run -n evaluate -d ./src/evaluate.py -d ./data/weatherAUS_processed.csv \\ -d ./src/model.py -d ./models/model.joblib \\ -M ./results/metrics.json \\ -o ./results/precision_recall_curve.png -o ./results/roc_curve.png \\ python3 ./src/evaluate.py ./data/weatherAUS_processed.csv ./src/model.py ./models/model.joblib You might notice that we are using the -M flag instead of the -o flag. This is important because now we can keep the metrics generated by every experiment. If we run dvc metrics show we can see how good was the experiment: $ dvc metrics show Path accuracy f1 precision recall results/metrics.json 0 .84973 0 .90747 0 .8719 0 .94607 Another import command is if we want to compare this experiment made in our branch to the model in production at the main branch we can do this by running dvc metrics diff : $ dvc metrics diff Path Metric Old New Change results/metrics.json accuracy 0 .84643 0 .84973 0 .0033 results/metrics.json f1 0 .9074 0 .90747 8e-05 results/metrics.json precision 0 .85554 0 .8719 0 .01636 results/metrics.json recall 0 .96594 0 .94607 -0.01987 The metrics configuration is saved at the dvc.yaml file. Control the experiment So now we have built a full machine learning experiment with three pipelines: DVC uses a Direct Acyclic Graph(DAG) to organize the relationships and dependencies between pipelines. This is very useful for visualizing the experiment process, especially when sharing it with your team. You can check the DAG just by running dvc dag : $ dvc dag +-------------------------+ | data/weatherAUS.csv.dvc | +-------------------------+ * * * +------------+ | preprocess | +------------+ ** ** ** ** * ** +-------+ * | train | ** +-------+ ** ** ** ** ** * * +----------+ | evaluate | +----------+ If you want to check any changes to the project's pipelines, just run: $ dvc status Reproducibility Either if you changed one stage and need to run it again or if you are reproducing someone's experiment for the first time, DVC helps you with that: Run this to reproduce the whole experiment: $ dvc repro Or if you want just to reproduce one of the stages, just let DVC know that by: $ dvc repro stage_name Info If you are using dvc repro for a second time, DVC will reproduce only those stages that changes have been made.","title":"Working with Pipelines"},{"location":"old/Versionamento/pipelines_dvc/#working-with-pipelines","text":"After learning how to version data or models using DVC it's time to build your experiment pipeline. Let's assume that we are using the last section dataset as a data source for training a classification model. Let's also consider that we have three stages in this experiment: Preprocessing your data(extract features...) Train the model Evaluate the model Here you should have in hands our scripts: preprocess.py , train.py , evaluate.py and model.py . Warning Just as in the last section, we will use the help of the template repository to explain and build DVC's pipelines. Feel free to use your scripts and create specific pipelines for your project needs.","title":"Working with Pipelines"},{"location":"old/Versionamento/pipelines_dvc/#creating-pipelines","text":"DVC builds a pipeline based on three components: Inputs, Outputs, and Command. So for the preprocessing stage, this would look like this: Inputs: weatherAUS.csv.csv and preprocess.py script Outputs: weatherAUS_processed.csv Command: python preprocess.py weatherAUS.csv So to create this stage of preprocessing, we use dvc run : dvc run -n preprocess \\ -d ./src/preprocess_data.py -d data/weatherAUS.csv \\ -o ./data/weatherAUS_processed.csv \\ python3 ./src/preprocess_data.py ./data/weatherAUS.csv We named this stage \"preprocess\" by using the flag -n . We also defined this stage inputs with the flag -d and the outputs with the flag -o . The command will always be the last piece of dvc run without any flag. Tip Output files are added to DVC control when reproducing a DVC stage. When finalizing your experiment remember to use dvc push to version not only the data used but those outputs generated from the experiment. The train stages would also be created using dvc run : dvc run -n train \\ -d ./src/train.py -d ./data/weatherAUS_processed.csv -d ./src/model.py \\ -o ./models/model.joblib \\ python3 ./src/train.py ./data/weatherAUS_processed.csv ./src/model.py 200 Warning The number 200 at dvc run above is related to our script function. If your are using your own script just ignore it. At this point, you might have noticed that two new files were created: dvc.yaml and dvc.lock . The first one will be responsible for saving what was described in each dvc run command. So if you wanna create or change a specific stage, it's possible to just edit dvc.yaml . Our current file would look like this: stages : preprocess : cmd : python3 ./src/preprocess_data.py ./data/weatherAUS.csv deps : - ./src/preprocess_data.py - data/weatherAUS.csv outs : - ./data/weatherAUS_processed.csv - ./data/features.csv train : cmd : python3 ./src/train.py ./data/weatherAUS_processed.csv ./src/model.py 200 deps : - ./data/weatherAUS_processed.csv - ./src/model.py - ./src/train.py outs : - ./models/model.joblib The second file created is dvc.lock . This is also a YAML file and its function is similar to .dvc files. Inside, we can find the path and a hash code for each file of each stage so DVC can track changes. Tracking these changes is important because now DVC will know when a stage needs to be rerun or not. Your currently pipeline looks likes this:","title":"Creating pipelines"},{"location":"old/Versionamento/pipelines_dvc/#saving-metrics","text":"Finally, let's create our last stage so we can evaluate our model: dvc run -n evaluate -d ./src/evaluate.py -d ./data/weatherAUS_processed.csv \\ -d ./src/model.py -d ./models/model.joblib \\ -M ./results/metrics.json \\ -o ./results/precision_recall_curve.png -o ./results/roc_curve.png \\ python3 ./src/evaluate.py ./data/weatherAUS_processed.csv ./src/model.py ./models/model.joblib You might notice that we are using the -M flag instead of the -o flag. This is important because now we can keep the metrics generated by every experiment. If we run dvc metrics show we can see how good was the experiment: $ dvc metrics show Path accuracy f1 precision recall results/metrics.json 0 .84973 0 .90747 0 .8719 0 .94607 Another import command is if we want to compare this experiment made in our branch to the model in production at the main branch we can do this by running dvc metrics diff : $ dvc metrics diff Path Metric Old New Change results/metrics.json accuracy 0 .84643 0 .84973 0 .0033 results/metrics.json f1 0 .9074 0 .90747 8e-05 results/metrics.json precision 0 .85554 0 .8719 0 .01636 results/metrics.json recall 0 .96594 0 .94607 -0.01987 The metrics configuration is saved at the dvc.yaml file.","title":"Saving metrics"},{"location":"old/Versionamento/pipelines_dvc/#control-the-experiment","text":"So now we have built a full machine learning experiment with three pipelines: DVC uses a Direct Acyclic Graph(DAG) to organize the relationships and dependencies between pipelines. This is very useful for visualizing the experiment process, especially when sharing it with your team. You can check the DAG just by running dvc dag : $ dvc dag +-------------------------+ | data/weatherAUS.csv.dvc | +-------------------------+ * * * +------------+ | preprocess | +------------+ ** ** ** ** * ** +-------+ * | train | ** +-------+ ** ** ** ** ** * * +----------+ | evaluate | +----------+ If you want to check any changes to the project's pipelines, just run: $ dvc status","title":"Control the experiment"},{"location":"old/Versionamento/pipelines_dvc/#reproducibility","text":"Either if you changed one stage and need to run it again or if you are reproducing someone's experiment for the first time, DVC helps you with that: Run this to reproduce the whole experiment: $ dvc repro Or if you want just to reproduce one of the stages, just let DVC know that by: $ dvc repro stage_name Info If you are using dvc repro for a second time, DVC will reproduce only those stages that changes have been made.","title":"Reproducibility"},{"location":"old/Workflow/","text":"Project Workflow In this section, we present a workflow example on how to create a new experiment in the project. This is also a step-by-step guide to the video presented on the home page. Note If you don't have a configured project already, go check out the Implementation Guide first. Setup your environment Cloning your repository and installing dependencies Let's start by cloning the project repository you will be working on. We need to make sure that we have installed all the dependencies necessary. Tip It's a good practice to create a virtual environment for each project you work. You can do that using venv or conda . git clone https://github.com/mlops-guide/dvc-gitactions.git cd dvc-gitactions pip3 install -r requirements.txt pre-commit install Updating the data and checking the project specifications Your environment is ready! So now let's: Download the data Check the project pipelines Reproduce the main experiment(Optional) See current metrics dvc pull dvc dag dvc repro dvc metrics show Info If you are confused with this DVC's commands, check out at the Implementation Guide the Versioning section . Working on a New Update Now that you are familiar with the project let's try a new experiment. Edit the necessary files First, you should edit those files affected for your experiment. Here we are changing our model from a Logistic Regression classifier to a Random Forest at model.py . model.py pipe = Pipeline ( [ ( \"scaler\" , StandardScaler ()), - ( \"LR\" , LogisticRegression ( random_state = 0 , max_iter = num_estimators )), + ( + \"RFC\" , + RandomForestClassifier ( + criterion = \"gini\" , + max_depth = 10 , + max_features = \"auto\" , + n_estimators = num_estimators , + ), + ), ] ) Info If you forgot how this project is organized and what each file is responsible for, go check out Tools and Project Structure . Create new branch and Reproduce pipelines Second, let's create a new branch at our repository to version this new experiment. After that, we can reproduce the experiment and see how the new metrics compare to the current model metrics. git checkout -b RandomForestClassifier dvc repro dvc metrics diff Note Observe that DVC avoided running the preprocess stage since our model change affected only the train and evaluate stages. To see more about DVC pipelines go check out Working with pipelines . Even though our experiment didn't improve our current model metrics, we will consider it good for production to demonstrate the rest of the workflow cycle. Test and Commit Our experiment is ready, now let's: Format our code with black Upload to our branch in our Github repository black . git add . dvc push git commit -m \"Random Forest Experiment\" git push origin RandomForestClassifier Info The tests and format checking which ran after the commit command were executed by pre-commit Experiment Deployment Pull request and automated report After uploading to our branch, we can now create a Pull Request at the Github Website. Info If forgot or want to know more about how this automated report was generated, go check out the Continuous Integration with CML and Github Actions Release and Watson Deployment Supposing our experiment was merged to the main branch, we can consider it ready for deployment. To do so, let's release a new version of the project using Github Website. After releasing the new version, CML and Github Actions will trigger a script responsible for deploying our model to Watson ML. Info If forgot or want to know more how this deployment happens, go check out the Continous Delivery with CML, Github Actions and Watson ML Monitoring Ending our workflow cycle, we can use IBM OpenScale tool to monitor the model in production. There we can create monitors for Drift, Accuracy and Fairness. We can also explain the model's predictions, understanding which feature had more weight in the decision and also see what changes would need to be made for the outcome to change. Info If forgot or want to know more how to monitor your model in production, go check out the Monitoring with IBM OpenScale","title":"Project Workflow"},{"location":"old/Workflow/#project-workflow","text":"In this section, we present a workflow example on how to create a new experiment in the project. This is also a step-by-step guide to the video presented on the home page. Note If you don't have a configured project already, go check out the Implementation Guide first.","title":"Project Workflow"},{"location":"old/Workflow/#setup-your-environment","text":"","title":"Setup your environment"},{"location":"old/Workflow/#cloning-your-repository-and-installing-dependencies","text":"Let's start by cloning the project repository you will be working on. We need to make sure that we have installed all the dependencies necessary. Tip It's a good practice to create a virtual environment for each project you work. You can do that using venv or conda . git clone https://github.com/mlops-guide/dvc-gitactions.git cd dvc-gitactions pip3 install -r requirements.txt pre-commit install","title":"Cloning your repository and installing dependencies"},{"location":"old/Workflow/#updating-the-data-and-checking-the-project-specifications","text":"Your environment is ready! So now let's: Download the data Check the project pipelines Reproduce the main experiment(Optional) See current metrics dvc pull dvc dag dvc repro dvc metrics show Info If you are confused with this DVC's commands, check out at the Implementation Guide the Versioning section .","title":"Updating the data and checking the project specifications"},{"location":"old/Workflow/#working-on-a-new-update","text":"Now that you are familiar with the project let's try a new experiment.","title":"Working on a New Update"},{"location":"old/Workflow/#edit-the-necessary-files","text":"First, you should edit those files affected for your experiment. Here we are changing our model from a Logistic Regression classifier to a Random Forest at model.py . model.py pipe = Pipeline ( [ ( \"scaler\" , StandardScaler ()), - ( \"LR\" , LogisticRegression ( random_state = 0 , max_iter = num_estimators )), + ( + \"RFC\" , + RandomForestClassifier ( + criterion = \"gini\" , + max_depth = 10 , + max_features = \"auto\" , + n_estimators = num_estimators , + ), + ), ] ) Info If you forgot how this project is organized and what each file is responsible for, go check out Tools and Project Structure .","title":"Edit the necessary files"},{"location":"old/Workflow/#create-new-branch-and-reproduce-pipelines","text":"Second, let's create a new branch at our repository to version this new experiment. After that, we can reproduce the experiment and see how the new metrics compare to the current model metrics. git checkout -b RandomForestClassifier dvc repro dvc metrics diff Note Observe that DVC avoided running the preprocess stage since our model change affected only the train and evaluate stages. To see more about DVC pipelines go check out Working with pipelines . Even though our experiment didn't improve our current model metrics, we will consider it good for production to demonstrate the rest of the workflow cycle.","title":"Create new branch and Reproduce pipelines"},{"location":"old/Workflow/#test-and-commit","text":"Our experiment is ready, now let's: Format our code with black Upload to our branch in our Github repository black . git add . dvc push git commit -m \"Random Forest Experiment\" git push origin RandomForestClassifier Info The tests and format checking which ran after the commit command were executed by pre-commit","title":"Test and Commit"},{"location":"old/Workflow/#experiment-deployment","text":"","title":"Experiment Deployment"},{"location":"old/Workflow/#pull-request-and-automated-report","text":"After uploading to our branch, we can now create a Pull Request at the Github Website. Info If forgot or want to know more about how this automated report was generated, go check out the Continuous Integration with CML and Github Actions","title":"Pull request and automated report"},{"location":"old/Workflow/#release-and-watson-deployment","text":"Supposing our experiment was merged to the main branch, we can consider it ready for deployment. To do so, let's release a new version of the project using Github Website. After releasing the new version, CML and Github Actions will trigger a script responsible for deploying our model to Watson ML. Info If forgot or want to know more how this deployment happens, go check out the Continous Delivery with CML, Github Actions and Watson ML","title":"Release and Watson Deployment"},{"location":"old/Workflow/#monitoring","text":"Ending our workflow cycle, we can use IBM OpenScale tool to monitor the model in production. There we can create monitors for Drift, Accuracy and Fairness. We can also explain the model's predictions, understanding which feature had more weight in the decision and also see what changes would need to be made for the outcome to change. Info If forgot or want to know more how to monitor your model in production, go check out the Monitoring with IBM OpenScale","title":"Monitoring"}]}